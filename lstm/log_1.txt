TRAIN: iteration: 0 - acc: 0.0 - loss: 9.21119499207 
TRAIN: iteration: 500 - acc: 0.0892499983311 - loss: 6.55092382431 
TRAIN: iteration: 0 - acc: 0.0 - loss: 9.50341796875 
TRAIN: iteration: 0 - acc: 0.0 - loss: 9.49677276611 
TRAIN: iteration: 500 - acc: 0.0765625014901 - loss: 6.85324573517 
TRAIN: iteration: 1000 - acc: 0.118062503636 - loss: 6.32542419434 
TRAIN: iteration: 1500 - acc: 0.172124996781 - loss: 5.86389827728 
TRAIN: iteration: 2000 - acc: 0.210374996066 - loss: 5.47026014328 
TRAIN: iteration: 2500 - acc: 0.225500002503 - loss: 5.29794216156 
TRAIN: iteration: 3000 - acc: 0.243874996901 - loss: 5.18293380737 
TRAIN: iteration: 3500 - acc: 0.262312501669 - loss: 4.96050882339 
TRAIN: iteration: 4000 - acc: 0.280187487602 - loss: 4.84935569763 
TRAIN: iteration: 4500 - acc: 0.287000000477 - loss: 4.7826128006 
TRAIN: iteration: 5000 - acc: 0.29675000906 - loss: 4.673391819 
TRAIN: iteration: 5500 - acc: 0.304374992847 - loss: 4.59875154495 
TRAIN: iteration: 6000 - acc: 0.314624994993 - loss: 4.51920032501 
TRAIN: iteration: 6500 - acc: 0.329499989748 - loss: 4.466319561 
TRAIN: iteration: 7000 - acc: 0.33575001359 - loss: 4.35945224762 
TRAIN: iteration: 7500 - acc: 0.339187502861 - loss: 4.35088014603 
TRAIN: iteration: 8000 - acc: 0.343062490225 - loss: 4.27924919128 
TRAIN: iteration: 8500 - acc: 0.353312492371 - loss: 4.23972892761 
TRAIN: iteration: 9000 - acc: 0.350125014782 - loss: 4.22500276566 
TRAIN: iteration: 9500 - acc: 0.351062506437 - loss: 4.19176673889 
TRAIN: iteration: 10000 - acc: 0.35931250453 - loss: 4.13590431213 
TRAIN: iteration: 10500 - acc: 0.363499999046 - loss: 4.07796144485 
TRAIN: iteration: 11000 - acc: 0.371562510729 - loss: 4.02961921692 
TRAIN: iteration: 11500 - acc: 0.369125008583 - loss: 4.06141281128 
TRAIN: iteration: 12000 - acc: 0.372937500477 - loss: 4.00921058655 
TRAIN: iteration: 12500 - acc: 0.379062503576 - loss: 3.9896736145 
TRAIN: iteration: 13000 - acc: 0.386750012636 - loss: 3.93834018707 
TRAIN: iteration: 13500 - acc: 0.38431251049 - loss: 3.9136030674 
TRAIN: iteration: 14000 - acc: 0.39050000906 - loss: 3.8742723465 
TRAIN: iteration: 14500 - acc: 0.387499988079 - loss: 3.89164710045 
TRAIN: iteration: 15000 - acc: 0.394562512636 - loss: 3.83802914619 
TRAIN: iteration: 15500 - acc: 0.394499987364 - loss: 3.82688641548 
TRAIN: iteration: 16000 - acc: 0.389874994755 - loss: 3.87994194031 
TRAIN: iteration: 16500 - acc: 0.407875001431 - loss: 3.81122493744 
TRAIN: iteration: 17000 - acc: 0.396312505007 - loss: 3.84448599815 
TRAIN: iteration: 17500 - acc: 0.394562512636 - loss: 3.84257459641 
TRAIN: iteration: 18000 - acc: 0.408687502146 - loss: 3.7383055687 
TRAIN: iteration: 18500 - acc: 0.407312512398 - loss: 3.71996426582 
TRAIN: iteration: 19000 - acc: 0.403687506914 - loss: 3.76596927643 
TRAIN: iteration: 19500 - acc: 0.415250003338 - loss: 3.70392155647 
TRAIN: iteration: 20000 - acc: 0.409812510014 - loss: 3.70846104622 
TRAIN: iteration: 20500 - acc: 0.413187503815 - loss: 3.70063185692 
TRAIN: iteration: 21000 - acc: 0.414875000715 - loss: 3.6683409214 
TRAIN: iteration: 21500 - acc: 0.419999986887 - loss: 3.61469888687 
TRAIN: iteration: 22000 - acc: 0.419499993324 - loss: 3.63914179802 
TRAIN: iteration: 22500 - acc: 0.417124986649 - loss: 3.63777399063 
TRAIN: iteration: 23000 - acc: 0.418875008821 - loss: 3.62768340111 
TRAIN: iteration: 23500 - acc: 0.419187486172 - loss: 3.61181855202 
TRAIN: iteration: 0 - acc: 0.5625 - loss: 3.01770591736 
TRAIN: iteration: 500 - acc: 0.410312503576 - loss: 3.61367321014 
TRAIN: iteration: 1000 - acc: 0.414437502623 - loss: 3.60297012329 
TRAIN: iteration: 1500 - acc: 0.409187495708 - loss: 3.56914973259 
TRAIN: iteration: 0 - acc: 0.0 - loss: 9.49878883362 
TRAIN: iteration: 500 - acc: 0.0753125026822 - loss: 6.84870243073 
TRAIN: iteration: 1000 - acc: 0.116125002503 - loss: 6.33075523376 
TRAIN: iteration: 1500 - acc: 0.175750002265 - loss: 5.85524511337 
TRAIN: iteration: 2000 - acc: 0.211750000715 - loss: 5.45563030243 
TRAIN: iteration: 2500 - acc: 0.228874996305 - loss: 5.2819480896 
TRAIN: iteration: 3000 - acc: 0.24518750608 - loss: 5.17525959015 
TRAIN: iteration: 3500 - acc: 0.263187497854 - loss: 4.95438814163 
TRAIN: iteration: 4000 - acc: 0.28112500906 - loss: 4.84810209274 
TRAIN: iteration: 4500 - acc: 0.284999996424 - loss: 4.77674818039 
TRAIN: iteration: 5000 - acc: 0.298624992371 - loss: 4.67106866837 
TRAIN: iteration: 5500 - acc: 0.306625008583 - loss: 4.59116125107 
TRAIN: iteration: 6000 - acc: 0.313812494278 - loss: 4.50790834427 
TRAIN: iteration: 6500 - acc: 0.329374998808 - loss: 4.45584726334 
TRAIN: iteration: 7000 - acc: 0.333562493324 - loss: 4.3502073288 
TRAIN: iteration: 7500 - acc: 0.338874995708 - loss: 4.33787679672 
TRAIN: iteration: 8000 - acc: 0.342687487602 - loss: 4.27341222763 
TRAIN: iteration: 8500 - acc: 0.353500008583 - loss: 4.22557353973 
TRAIN: iteration: 9000 - acc: 0.350812494755 - loss: 4.21882534027 
TRAIN: iteration: 9500 - acc: 0.355125010014 - loss: 4.17139101028 
TRAIN: iteration: 10000 - acc: 0.358187496662 - loss: 4.12779998779 
TRAIN: iteration: 10500 - acc: 0.36568748951 - loss: 4.07090950012 
TRAIN: iteration: 11000 - acc: 0.373374998569 - loss: 4.02587604523 
TRAIN: iteration: 11500 - acc: 0.368624985218 - loss: 4.05325508118 
TRAIN: iteration: 12000 - acc: 0.372312486172 - loss: 3.9949862957 
TRAIN: iteration: 12500 - acc: 0.379687488079 - loss: 3.98413920403 
TRAIN: iteration: 13000 - acc: 0.387437492609 - loss: 3.9299120903 
TRAIN: iteration: 13500 - acc: 0.387499988079 - loss: 3.90518951416 
TRAIN: iteration: 14000 - acc: 0.391812503338 - loss: 3.86518287659 
TRAIN: iteration: 14500 - acc: 0.387749999762 - loss: 3.88853383064 
TRAIN: iteration: 15000 - acc: 0.396250009537 - loss: 3.82555913925 
TRAIN: iteration: 15500 - acc: 0.394687503576 - loss: 3.82166147232 
TRAIN: iteration: 16000 - acc: 0.393875002861 - loss: 3.87018704414 
TRAIN: iteration: 16500 - acc: 0.407875001431 - loss: 3.80943346024 
TRAIN: iteration: 17000 - acc: 0.398750007153 - loss: 3.83241033554 
TRAIN: iteration: 17500 - acc: 0.397562503815 - loss: 3.84081745148 
TRAIN: iteration: 18000 - acc: 0.409562498331 - loss: 3.73138070107 
TRAIN: iteration: 18500 - acc: 0.406687498093 - loss: 3.71771359444 
TRAIN: iteration: 19000 - acc: 0.405624985695 - loss: 3.75434374809 
TRAIN: iteration: 19500 - acc: 0.414562493563 - loss: 3.7010512352 
TRAIN: iteration: 20000 - acc: 0.412874996662 - loss: 3.69940376282 
TRAIN: iteration: 20500 - acc: 0.415749996901 - loss: 3.68354153633 
TRAIN: iteration: 21000 - acc: 0.417750000954 - loss: 3.6576757431 
TRAIN: iteration: 21500 - acc: 0.42206248641 - loss: 3.61925840378 
TRAIN: iteration: 22000 - acc: 0.418000012636 - loss: 3.6370549202 
TRAIN: iteration: 22500 - acc: 0.417374998331 - loss: 3.63261699677 
TRAIN: iteration: 23000 - acc: 0.417937487364 - loss: 3.62012696266 
TRAIN: iteration: 23500 - acc: 0.419625014067 - loss: 3.60453820229 
TRAIN: iteration: 24000 - acc: 0.423750013113 - loss: 3.60830998421 
TRAIN: iteration: 24500 - acc: 0.429937511683 - loss: 3.51328372955 
TRAIN: iteration: 25000 - acc: 0.426062494516 - loss: 3.5648868084 
TRAIN: iteration: 25500 - acc: 0.430312514305 - loss: 3.53285431862 
TRAIN: iteration: 26000 - acc: 0.427625000477 - loss: 3.53586864471 
TRAIN: iteration: 26500 - acc: 0.435312509537 - loss: 3.50731277466 
TRAIN: iteration: 27000 - acc: 0.429312497377 - loss: 3.53062701225 
TRAIN: iteration: 27500 - acc: 0.433999985456 - loss: 3.50935935974 
TRAIN: iteration: 28000 - acc: 0.443625003099 - loss: 3.4691631794 
TRAIN: iteration: 28500 - acc: 0.435562491417 - loss: 3.4736495018 
TRAIN: iteration: 29000 - acc: 0.42962500453 - loss: 3.48285150528 
TRAIN: iteration: 29500 - acc: 0.439937502146 - loss: 3.44626307487 
TRAIN: iteration: 30000 - acc: 0.440124988556 - loss: 3.4287352562 
TRAIN: iteration: 30500 - acc: 0.440750002861 - loss: 3.42459368706 
TRAIN: iteration: 31000 - acc: 0.438250005245 - loss: 3.47908353806 
TRAIN: iteration: 31500 - acc: 0.431562513113 - loss: 3.54200339317 
TRAIN: iteration: 32000 - acc: 0.433250010014 - loss: 3.59471797943 
TRAIN: iteration: 32500 - acc: 0.430375009775 - loss: 3.57632017136 
TRAIN: iteration: 33000 - acc: 0.441437512636 - loss: 3.52366948128 
TRAIN: iteration: 33500 - acc: 0.442312508821 - loss: 3.52034449577 
TRAIN: iteration: 34000 - acc: 0.433062493801 - loss: 3.52849411964 
TRAIN: iteration: 34500 - acc: 0.437000006437 - loss: 3.5224404335 
TRAIN: iteration: 35000 - acc: 0.443062514067 - loss: 3.47380542755 
TRAIN: iteration: 35500 - acc: 0.444062501192 - loss: 3.43290472031 
TRAIN: iteration: 36000 - acc: 0.442187488079 - loss: 3.45341444016 
TRAIN: iteration: 36500 - acc: 0.451687514782 - loss: 3.40443301201 
TRAIN: iteration: 37000 - acc: 0.444000005722 - loss: 3.44215917587 
TRAIN: iteration: 0 - acc: 0.5 - loss: 2.79938030243 
TRAIN: iteration: 500 - acc: 0.446687489748 - loss: 3.35585355759 
TRAIN: iteration: 1000 - acc: 0.44543749094 - loss: 3.34714913368 
TRAIN: iteration: 1500 - acc: 0.449312508106 - loss: 3.31679153442 
TRAIN: iteration: 2000 - acc: 0.457062512636 - loss: 3.2605984211 
TRAIN: iteration: 2500 - acc: 0.453437507153 - loss: 3.2632791996 
TRAIN: iteration: 3000 - acc: 0.453500002623 - loss: 3.29808831215 
TRAIN: iteration: 3500 - acc: 0.45331248641 - loss: 3.2411942482 
TRAIN: iteration: 4000 - acc: 0.460312485695 - loss: 3.24447655678 
TRAIN: iteration: 4500 - acc: 0.458999991417 - loss: 3.23003125191 
TRAIN: iteration: 5000 - acc: 0.464062511921 - loss: 3.18640351295 
TRAIN: iteration: 5500 - acc: 0.469187498093 - loss: 3.15778446198 
TRAIN: iteration: 6000 - acc: 0.465562492609 - loss: 3.16416501999 
TRAIN: iteration: 6500 - acc: 0.471937507391 - loss: 3.16408109665 
TRAIN: iteration: 7000 - acc: 0.475625008345 - loss: 3.12235403061 
TRAIN: iteration: 7500 - acc: 0.473312497139 - loss: 3.13180780411 
TRAIN: iteration: 8000 - acc: 0.469624996185 - loss: 3.10146284103 
TRAIN: iteration: 8500 - acc: 0.474500000477 - loss: 3.09899616241 
TRAIN: iteration: 9000 - acc: 0.472375005484 - loss: 3.10251617432 
TRAIN: iteration: 9500 - acc: 0.471187502146 - loss: 3.08960556984 
TRAIN: iteration: 10000 - acc: 0.473500013351 - loss: 3.08535575867 
TRAIN: iteration: 10500 - acc: 0.481875002384 - loss: 3.03818106651 
TRAIN: iteration: 11000 - acc: 0.481249988079 - loss: 3.02359080315 
TRAIN: iteration: 11500 - acc: 0.477625012398 - loss: 3.06039118767 
TRAIN: iteration: 12000 - acc: 0.484375 - loss: 3.0175139904 
TRAIN: iteration: 12500 - acc: 0.480187505484 - loss: 3.02375984192 
TRAIN: iteration: 13000 - acc: 0.490562498569 - loss: 2.98921966553 
TRAIN: iteration: 13500 - acc: 0.488062500954 - loss: 2.9818341732 
TRAIN: iteration: 14000 - acc: 0.492875009775 - loss: 2.94543600082 
TRAIN: iteration: 14500 - acc: 0.487562507391 - loss: 2.96798729897 
TRAIN: iteration: 15000 - acc: 0.494812488556 - loss: 2.92868256569 
TRAIN: iteration: 15500 - acc: 0.488375008106 - loss: 2.92771625519 
TRAIN: iteration: 16000 - acc: 0.4921875 - loss: 2.95595693588 
TRAIN: iteration: 16500 - acc: 0.500687479973 - loss: 2.90809416771 
TRAIN: iteration: 17000 - acc: 0.494749993086 - loss: 2.9297606945 
TRAIN: iteration: 17500 - acc: 0.489187508821 - loss: 2.94672489166 
TRAIN: iteration: 18000 - acc: 0.500750005245 - loss: 2.86261820793 
TRAIN: iteration: 18500 - acc: 0.499749988317 - loss: 2.8724091053 
TRAIN: iteration: 19000 - acc: 0.497124999762 - loss: 2.89201688766 
TRAIN: iteration: 19500 - acc: 0.503875017166 - loss: 2.86745858192 
TRAIN: iteration: 20000 - acc: 0.499624997377 - loss: 2.87363529205 
TRAIN: iteration: 20500 - acc: 0.504437506199 - loss: 2.87372303009 
TRAIN: iteration: 21000 - acc: 0.502187490463 - loss: 2.85292625427 
TRAIN: iteration: 21500 - acc: 0.506250023842 - loss: 2.82516026497 
TRAIN: iteration: 22000 - acc: 0.504937529564 - loss: 2.84954714775 
TRAIN: iteration: 22500 - acc: 0.5078125 - loss: 2.84602284431 
TRAIN: iteration: 23000 - acc: 0.508125007153 - loss: 2.82232189178 
TRAIN: iteration: 23500 - acc: 0.504625022411 - loss: 2.83401727676 
TRAIN: iteration: 24000 - acc: 0.511062502861 - loss: 2.83370256424 
TRAIN: iteration: 24500 - acc: 0.514812529087 - loss: 2.75217032433 
TRAIN: iteration: 25000 - acc: 0.512875020504 - loss: 2.79439353943 
TRAIN: iteration: 25500 - acc: 0.513875007629 - loss: 2.77432203293 
TRAIN: iteration: 26000 - acc: 0.513249993324 - loss: 2.79140663147 
TRAIN: iteration: 26500 - acc: 0.514937520027 - loss: 2.76048445702 
TRAIN: iteration: 27000 - acc: 0.513875007629 - loss: 2.77779889107 
TRAIN: iteration: 27500 - acc: 0.516499996185 - loss: 2.76205849648 
TRAIN: iteration: 28000 - acc: 0.521312475204 - loss: 2.74660730362 
TRAIN: iteration: 28500 - acc: 0.518937528133 - loss: 2.73987460136 
TRAIN: iteration: 29000 - acc: 0.514812529087 - loss: 2.73989057541 
TRAIN: iteration: 29500 - acc: 0.517374992371 - loss: 2.72709727287 
TRAIN: iteration: 30000 - acc: 0.520062506199 - loss: 2.70573687553 
TRAIN: iteration: 30500 - acc: 0.521437525749 - loss: 2.70086145401 
TRAIN: iteration: 31000 - acc: 0.52193748951 - loss: 2.75187158585 
TRAIN: iteration: 31500 - acc: 0.515187501907 - loss: 2.80171918869 
TRAIN: iteration: 32000 - acc: 0.514312505722 - loss: 2.83068704605 
TRAIN: iteration: 32500 - acc: 0.507375001907 - loss: 2.83870911598 
TRAIN: iteration: 33000 - acc: 0.511437475681 - loss: 2.80162739754 
TRAIN: iteration: 33500 - acc: 0.519312500954 - loss: 2.79700255394 
TRAIN: iteration: 34000 - acc: 0.512687504292 - loss: 2.79974222183 
TRAIN: iteration: 34500 - acc: 0.514062523842 - loss: 2.81293296814 
TRAIN: iteration: 35000 - acc: 0.517562508583 - loss: 2.78384757042 
TRAIN: iteration: 35500 - acc: 0.522875010967 - loss: 2.72425246239 
TRAIN: iteration: 36000 - acc: 0.517250001431 - loss: 2.75050139427 
TRAIN: iteration: 36500 - acc: 0.529250025749 - loss: 2.71558094025 
TRAIN: iteration: 37000 - acc: 0.51712501049 - loss: 2.7432281971 
TRAIN: iteration: 0 - acc: 0.75 - loss: 2.10814666748 
TRAIN: iteration: 500 - acc: 0.518687486649 - loss: 2.66092729568 
TRAIN: iteration: 1000 - acc: 0.527999997139 - loss: 2.65236496925 
TRAIN: iteration: 1500 - acc: 0.529500007629 - loss: 2.62008690834 
TRAIN: iteration: 2000 - acc: 0.533375024796 - loss: 2.58933401108 
TRAIN: iteration: 2500 - acc: 0.530375003815 - loss: 2.59428858757 
TRAIN: iteration: 3000 - acc: 0.536812484264 - loss: 2.61864995956 
TRAIN: iteration: 3500 - acc: 0.531875014305 - loss: 2.57665681839 
TRAIN: iteration: 4000 - acc: 0.532999992371 - loss: 2.5829501152 
TRAIN: iteration: 4500 - acc: 0.539749979973 - loss: 2.55995106697 
TRAIN: iteration: 5000 - acc: 0.542999982834 - loss: 2.51551413536 
TRAIN: iteration: 5500 - acc: 0.545062482357 - loss: 2.50304341316 
TRAIN: iteration: 6000 - acc: 0.547437489033 - loss: 2.49930644035 
TRAIN: iteration: 6500 - acc: 0.548749983311 - loss: 2.50963521004 
TRAIN: iteration: 7000 - acc: 0.550187528133 - loss: 2.46977925301 
TRAIN: iteration: 7500 - acc: 0.544312477112 - loss: 2.48983836174 
TRAIN: iteration: 8000 - acc: 0.548874974251 - loss: 2.45931148529 
TRAIN: iteration: 8500 - acc: 0.551562488079 - loss: 2.44673013687 
TRAIN: iteration: 9000 - acc: 0.551062524319 - loss: 2.4432618618 
TRAIN: iteration: 9500 - acc: 0.547375023365 - loss: 2.44738030434 
TRAIN: iteration: 10000 - acc: 0.549312472343 - loss: 2.45592164993 
TRAIN: iteration: 10500 - acc: 0.558749973774 - loss: 2.39939689636 
TRAIN: iteration: 11000 - acc: 0.558375000954 - loss: 2.39044117928 
TRAIN: iteration: 11500 - acc: 0.557062506676 - loss: 2.41188192368 
TRAIN: iteration: 12000 - acc: 0.56099998951 - loss: 2.37942194939 
TRAIN: iteration: 12500 - acc: 0.559937477112 - loss: 2.37493467331 
TRAIN: iteration: 13000 - acc: 0.565249979496 - loss: 2.34835672379 
TRAIN: iteration: 13500 - acc: 0.561874985695 - loss: 2.34133696556 
TRAIN: iteration: 14000 - acc: 0.568562507629 - loss: 2.328114748 
TRAIN: iteration: 14500 - acc: 0.566749989986 - loss: 2.31948781013 
TRAIN: iteration: 15000 - acc: 0.571937501431 - loss: 2.28520750999 
TRAIN: iteration: 15500 - acc: 0.571062505245 - loss: 2.29354786873 
TRAIN: iteration: 16000 - acc: 0.574249982834 - loss: 2.29370212555 
TRAIN: iteration: 16500 - acc: 0.579312503338 - loss: 2.26645374298 
TRAIN: iteration: 17000 - acc: 0.575124979019 - loss: 2.27086138725 
TRAIN: iteration: 17500 - acc: 0.566375017166 - loss: 2.29412603378 
TRAIN: iteration: 18000 - acc: 0.579187512398 - loss: 2.2321369648 
TRAIN: iteration: 18500 - acc: 0.581375002861 - loss: 2.24623823166 
TRAIN: iteration: 19000 - acc: 0.579999983311 - loss: 2.23765444756 
TRAIN: iteration: 19500 - acc: 0.584749996662 - loss: 2.22917628288 
TRAIN: iteration: 20000 - acc: 0.583312511444 - loss: 2.22533988953 
TRAIN: iteration: 20500 - acc: 0.584937512875 - loss: 2.23421955109 
TRAIN: iteration: 21000 - acc: 0.581875026226 - loss: 2.221565485 
TRAIN: iteration: 21500 - acc: 0.589187502861 - loss: 2.19013881683 
TRAIN: iteration: 22000 - acc: 0.583249986172 - loss: 2.21852684021 
TRAIN: iteration: 22500 - acc: 0.584937512875 - loss: 2.20874810219 
TRAIN: iteration: 23000 - acc: 0.58568751812 - loss: 2.17627930641 
TRAIN: iteration: 23500 - acc: 0.583562493324 - loss: 2.20466923714 
TRAIN: iteration: 24000 - acc: 0.589250028133 - loss: 2.19104933739 
TRAIN: iteration: 24500 - acc: 0.589874982834 - loss: 2.13267326355 
TRAIN: iteration: 25000 - acc: 0.591875016689 - loss: 2.16440486908 
TRAIN: iteration: 25500 - acc: 0.595624983311 - loss: 2.13135552406 
TRAIN: iteration: 26000 - acc: 0.595562517643 - loss: 2.14952731133 
TRAIN: iteration: 26500 - acc: 0.595937490463 - loss: 2.11380982399 
TRAIN: iteration: 27000 - acc: 0.594312489033 - loss: 2.13700675964 
TRAIN: iteration: 27500 - acc: 0.598687529564 - loss: 2.12821507454 
TRAIN: iteration: 28000 - acc: 0.60006248951 - loss: 2.12911963463 
TRAIN: iteration: 28500 - acc: 0.598874986172 - loss: 2.11095285416 
TRAIN: iteration: 29000 - acc: 0.603624999523 - loss: 2.09536027908 
TRAIN: iteration: 29500 - acc: 0.599562525749 - loss: 2.10218095779 
TRAIN: iteration: 30000 - acc: 0.603375017643 - loss: 2.06858944893 
TRAIN: iteration: 30500 - acc: 0.602562487125 - loss: 2.07454204559 
TRAIN: iteration: 31000 - acc: 0.600437521935 - loss: 2.11534357071 
TRAIN: iteration: 31500 - acc: 0.600562512875 - loss: 2.1361451149 
TRAIN: iteration: 32000 - acc: 0.601062476635 - loss: 2.14382076263 
TRAIN: iteration: 32500 - acc: 0.591499984264 - loss: 2.1580414772 
TRAIN: iteration: 33000 - acc: 0.596624970436 - loss: 2.14536333084 
TRAIN: iteration: 33500 - acc: 0.602187514305 - loss: 2.13888311386 
TRAIN: iteration: 34000 - acc: 0.597187519073 - loss: 2.13724660873 
TRAIN: iteration: 34500 - acc: 0.597249984741 - loss: 2.14481401443 
TRAIN: iteration: 35000 - acc: 0.598625004292 - loss: 2.14868497849 
TRAIN: iteration: 35500 - acc: 0.606312513351 - loss: 2.08826756477 
TRAIN: iteration: 36000 - acc: 0.603375017643 - loss: 2.09641218185 
TRAIN: iteration: 36500 - acc: 0.611625015736 - loss: 2.07288432121 
TRAIN: iteration: 37000 - acc: 0.604875028133 - loss: 2.09767913818 
TRAIN: iteration: 0 - acc: 0.78125 - loss: 1.653632164 
TRAIN: iteration: 500 - acc: 0.610249996185 - loss: 2.00820302963 
TRAIN: iteration: 1000 - acc: 0.614687502384 - loss: 1.99871969223 
TRAIN: iteration: 1500 - acc: 0.615249991417 - loss: 1.98034906387 
TRAIN: iteration: 2000 - acc: 0.62362498045 - loss: 1.97006750107 
TRAIN: iteration: 2500 - acc: 0.622187495232 - loss: 1.96617901325 
TRAIN: iteration: 3000 - acc: 0.626562476158 - loss: 1.96616911888 
TRAIN: iteration: 3500 - acc: 0.617937505245 - loss: 1.9633795023 
TRAIN: iteration: 4000 - acc: 0.619000017643 - loss: 1.95894467831 
TRAIN: iteration: 4500 - acc: 0.628624975681 - loss: 1.93719351292 
TRAIN: iteration: 5000 - acc: 0.634937524796 - loss: 1.89370465279 
TRAIN: iteration: 5500 - acc: 0.6328125 - loss: 1.88992869854 
TRAIN: iteration: 6000 - acc: 0.640187501907 - loss: 1.87611436844 
TRAIN: iteration: 6500 - acc: 0.633812487125 - loss: 1.88936960697 
TRAIN: iteration: 7000 - acc: 0.638875007629 - loss: 1.86515295506 
TRAIN: iteration: 7500 - acc: 0.634937524796 - loss: 1.88110113144 
TRAIN: iteration: 8000 - acc: 0.640124976635 - loss: 1.84714257717 
TRAIN: iteration: 8500 - acc: 0.643499970436 - loss: 1.83189368248 
TRAIN: iteration: 9000 - acc: 0.641624987125 - loss: 1.8372477293 
TRAIN: iteration: 9500 - acc: 0.640124976635 - loss: 1.82807898521 
TRAIN: iteration: 10000 - acc: 0.637187480927 - loss: 1.86024808884 
TRAIN: iteration: 10500 - acc: 0.651312470436 - loss: 1.79403269291 
TRAIN: iteration: 11000 - acc: 0.648625016212 - loss: 1.78852796555 
TRAIN: iteration: 11500 - acc: 0.646562516689 - loss: 1.80158126354 
TRAIN: iteration: 12000 - acc: 0.65637499094 - loss: 1.77090919018 
TRAIN: iteration: 12500 - acc: 0.654500007629 - loss: 1.76585114002 
TRAIN: iteration: 13000 - acc: 0.655562520027 - loss: 1.7450504303 
TRAIN: iteration: 13500 - acc: 0.653687477112 - loss: 1.74858987331 
TRAIN: iteration: 14000 - acc: 0.65750002861 - loss: 1.73715472221 
TRAIN: iteration: 14500 - acc: 0.665062487125 - loss: 1.70113384724 
TRAIN: iteration: 15000 - acc: 0.667375028133 - loss: 1.69524872303 
TRAIN: iteration: 15500 - acc: 0.662999987602 - loss: 1.69701075554 
TRAIN: iteration: 16000 - acc: 0.671937525272 - loss: 1.6769951582 
TRAIN: iteration: 16500 - acc: 0.674437522888 - loss: 1.66824018955 
TRAIN: iteration: 17000 - acc: 0.664624989033 - loss: 1.66370117664 
TRAIN: iteration: 17500 - acc: 0.665374994278 - loss: 1.68522584438 
TRAIN: iteration: 18000 - acc: 0.673187494278 - loss: 1.64522945881 
TRAIN: iteration: 18500 - acc: 0.673312485218 - loss: 1.65485501289 
TRAIN: iteration: 19000 - acc: 0.676312506199 - loss: 1.6348272562 
TRAIN: iteration: 19500 - acc: 0.678250014782 - loss: 1.63578724861 
TRAIN: iteration: 20000 - acc: 0.675312519073 - loss: 1.63327097893 
TRAIN: iteration: 20500 - acc: 0.675812482834 - loss: 1.64107859135 
TRAIN: iteration: 21000 - acc: 0.67212498188 - loss: 1.64863669872 
TRAIN: iteration: 21500 - acc: 0.683375000954 - loss: 1.60892403126 
TRAIN: iteration: 22000 - acc: 0.673624992371 - loss: 1.62729823589 
TRAIN: iteration: 22500 - acc: 0.6796875 - loss: 1.60734009743 
TRAIN: iteration: 23000 - acc: 0.684125006199 - loss: 1.58226191998 
TRAIN: iteration: 23500 - acc: 0.680249989033 - loss: 1.61123108864 
TRAIN: iteration: 24000 - acc: 0.684187471867 - loss: 1.59991240501 
TRAIN: iteration: 24500 - acc: 0.685062527657 - loss: 1.55910551548 
TRAIN: iteration: 25000 - acc: 0.686562478542 - loss: 1.59061443806 
TRAIN: iteration: 25500 - acc: 0.689750015736 - loss: 1.55074977875 
TRAIN: iteration: 26000 - acc: 0.694999992847 - loss: 1.55658996105 
TRAIN: iteration: 26500 - acc: 0.693062484264 - loss: 1.53496289253 
TRAIN: iteration: 27000 - acc: 0.691062510014 - loss: 1.55098462105 
TRAIN: iteration: 27500 - acc: 0.690937519073 - loss: 1.54598069191 
TRAIN: iteration: 28000 - acc: 0.692499995232 - loss: 1.5634765625 
TRAIN: iteration: 28500 - acc: 0.690812528133 - loss: 1.53284418583 
TRAIN: iteration: 29000 - acc: 0.699062526226 - loss: 1.51832997799 
TRAIN: iteration: 29500 - acc: 0.69543749094 - loss: 1.52225005627 
TRAIN: iteration: 30000 - acc: 0.702562510967 - loss: 1.50264501572 
TRAIN: iteration: 30500 - acc: 0.702562510967 - loss: 1.50785887241 
TRAIN: iteration: 31000 - acc: 0.694750010967 - loss: 1.54663550854 
TRAIN: iteration: 31500 - acc: 0.701312482357 - loss: 1.53530347347 
TRAIN: iteration: 32000 - acc: 0.699625015259 - loss: 1.52440285683 
TRAIN: iteration: 32500 - acc: 0.691874980927 - loss: 1.54268217087 
TRAIN: iteration: 33000 - acc: 0.692187488079 - loss: 1.5514023304 
TRAIN: iteration: 33500 - acc: 0.702062487602 - loss: 1.53633499146 
TRAIN: iteration: 34000 - acc: 0.698374986649 - loss: 1.54077386856 
TRAIN: iteration: 34500 - acc: 0.694249987602 - loss: 1.5444662571 
TRAIN: iteration: 35000 - acc: 0.696312487125 - loss: 1.56512963772 
TRAIN: iteration: 35500 - acc: 0.701499998569 - loss: 1.51077878475 
TRAIN: iteration: 36000 - acc: 0.700812518597 - loss: 1.51876688004 
TRAIN: iteration: 36500 - acc: 0.704437494278 - loss: 1.49939906597 
TRAIN: iteration: 37000 - acc: 0.700812518597 - loss: 1.51977860928 
TRAIN: iteration: 0 - acc: 0.78125 - loss: 1.44159436226 
TRAIN: iteration: 500 - acc: 0.705937504768 - loss: 1.46451377869 
TRAIN: iteration: 1000 - acc: 0.714437484741 - loss: 1.42896652222 
TRAIN: iteration: 1500 - acc: 0.714999973774 - loss: 1.43440234661 
TRAIN: iteration: 2000 - acc: 0.712999999523 - loss: 1.4315983057 
TRAIN: iteration: 2500 - acc: 0.713937520981 - loss: 1.42509937286 
TRAIN: iteration: 3000 - acc: 0.722812473774 - loss: 1.40509259701 
TRAIN: iteration: 3500 - acc: 0.715937495232 - loss: 1.42374384403 
TRAIN: iteration: 4000 - acc: 0.71724998951 - loss: 1.40781283379 
TRAIN: iteration: 4500 - acc: 0.71737498045 - loss: 1.40556740761 
TRAIN: iteration: 5000 - acc: 0.726750016212 - loss: 1.37346196175 
TRAIN: iteration: 5500 - acc: 0.724500000477 - loss: 1.3712233305 
TRAIN: iteration: 6000 - acc: 0.730499982834 - loss: 1.34404277802 
TRAIN: iteration: 6500 - acc: 0.732937514782 - loss: 1.35699462891 
TRAIN: iteration: 7000 - acc: 0.731812477112 - loss: 1.3494809866 
TRAIN: iteration: 7500 - acc: 0.723562479019 - loss: 1.37006998062 
TRAIN: iteration: 8000 - acc: 0.735812485218 - loss: 1.33019626141 
TRAIN: iteration: 8500 - acc: 0.738250017166 - loss: 1.3190356493 
TRAIN: iteration: 9000 - acc: 0.736062526703 - loss: 1.33608734608 
TRAIN: iteration: 9500 - acc: 0.738250017166 - loss: 1.31746947765 
TRAIN: iteration: 10000 - acc: 0.731124997139 - loss: 1.35166358948 
TRAIN: iteration: 10500 - acc: 0.740125000477 - loss: 1.29399251938 
TRAIN: iteration: 11000 - acc: 0.738874971867 - loss: 1.3068010807 
TRAIN: iteration: 11500 - acc: 0.742687523365 - loss: 1.30183172226 
TRAIN: iteration: 12000 - acc: 0.74356251955 - loss: 1.28444683552 
TRAIN: iteration: 12500 - acc: 0.750999987125 - loss: 1.26382768154 
TRAIN: iteration: 13000 - acc: 0.748187482357 - loss: 1.2654531002 
TRAIN: iteration: 13500 - acc: 0.746062517166 - loss: 1.26008892059 
TRAIN: iteration: 14000 - acc: 0.746749997139 - loss: 1.26160728931 
TRAIN: iteration: 14500 - acc: 0.75793749094 - loss: 1.21675443649 
TRAIN: iteration: 15000 - acc: 0.754562497139 - loss: 1.23323082924 
TRAIN: iteration: 15500 - acc: 0.752624988556 - loss: 1.22993326187 
TRAIN: iteration: 16000 - acc: 0.761812508106 - loss: 1.19935083389 
TRAIN: iteration: 16500 - acc: 0.759625017643 - loss: 1.1939021349 
TRAIN: iteration: 17000 - acc: 0.75743752718 - loss: 1.19632196426 
TRAIN: iteration: 17500 - acc: 0.755750000477 - loss: 1.2163541317 
TRAIN: iteration: 18000 - acc: 0.760500013828 - loss: 1.1852657795 
TRAIN: iteration: 18500 - acc: 0.757375001907 - loss: 1.20064258575 
TRAIN: iteration: 19000 - acc: 0.758750021458 - loss: 1.17927515507 
TRAIN: iteration: 19500 - acc: 0.765312492847 - loss: 1.19283008575 
TRAIN: iteration: 20000 - acc: 0.760687470436 - loss: 1.18993532658 
TRAIN: iteration: 20500 - acc: 0.762624979019 - loss: 1.18934464455 
TRAIN: iteration: 21000 - acc: 0.757062494755 - loss: 1.19583761692 
TRAIN: iteration: 21500 - acc: 0.761437475681 - loss: 1.18260121346 
TRAIN: iteration: 22000 - acc: 0.760562479496 - loss: 1.17991065979 
TRAIN: iteration: 22500 - acc: 0.763062477112 - loss: 1.17661333084 
TRAIN: iteration: 23000 - acc: 0.767062485218 - loss: 1.15661621094 
TRAIN: iteration: 23500 - acc: 0.764312505722 - loss: 1.18479561806 
TRAIN: iteration: 24000 - acc: 0.767750024796 - loss: 1.16750061512 
TRAIN: iteration: 24500 - acc: 0.768937528133 - loss: 1.1315805912 
TRAIN: iteration: 25000 - acc: 0.769312500954 - loss: 1.16347444057 
TRAIN: iteration: 25500 - acc: 0.769375026226 - loss: 1.13212037086 
TRAIN: iteration: 26000 - acc: 0.776624977589 - loss: 1.12842845917 
TRAIN: iteration: 26500 - acc: 0.774874985218 - loss: 1.12349736691 
TRAIN: iteration: 27000 - acc: 0.775499999523 - loss: 1.12340867519 
TRAIN: iteration: 27500 - acc: 0.772750020027 - loss: 1.13951265812 
TRAIN: iteration: 28000 - acc: 0.770749986172 - loss: 1.15176022053 
TRAIN: iteration: 28500 - acc: 0.772562503815 - loss: 1.11638271809 
TRAIN: iteration: 29000 - acc: 0.778625011444 - loss: 1.1036247015 
TRAIN: iteration: 29500 - acc: 0.777437508106 - loss: 1.10146141052 
TRAIN: iteration: 30000 - acc: 0.780499994755 - loss: 1.09361481667 
TRAIN: iteration: 30500 - acc: 0.778062522411 - loss: 1.10049760342 
TRAIN: iteration: 31000 - acc: 0.775562524796 - loss: 1.14180076122 
TRAIN: iteration: 31500 - acc: 0.781875014305 - loss: 1.09659004211 
TRAIN: iteration: 32000 - acc: 0.781187474728 - loss: 1.10087227821 
TRAIN: iteration: 32500 - acc: 0.776125013828 - loss: 1.11060404778 
TRAIN: iteration: 33000 - acc: 0.776687502861 - loss: 1.12373650074 
TRAIN: iteration: 33500 - acc: 0.778687477112 - loss: 1.1290910244 
TRAIN: iteration: 34000 - acc: 0.775250017643 - loss: 1.12781512737 
TRAIN: iteration: 34500 - acc: 0.775749981403 - loss: 1.12610912323 
TRAIN: iteration: 35000 - acc: 0.777125000954 - loss: 1.1385833025 
TRAIN: iteration: 35500 - acc: 0.779062509537 - loss: 1.10865962505 
TRAIN: iteration: 36000 - acc: 0.781062483788 - loss: 1.12288033962 
TRAIN: iteration: 36500 - acc: 0.782562494278 - loss: 1.10077571869 
TRAIN: iteration: 37000 - acc: 0.780624985695 - loss: 1.10247254372 
TRAIN: iteration: 0 - acc: 0.84375 - loss: 0.918074131012 
TRAIN: iteration: 500 - acc: 0.784937500954 - loss: 1.08006930351 
TRAIN: iteration: 1000 - acc: 0.791437506676 - loss: 1.04147565365 
TRAIN: iteration: 1500 - acc: 0.786875009537 - loss: 1.07678282261 
TRAIN: iteration: 2000 - acc: 0.789250016212 - loss: 1.05297338963 
TRAIN: iteration: 2500 - acc: 0.791124999523 - loss: 1.06130504608 
TRAIN: iteration: 3000 - acc: 0.798687517643 - loss: 1.03312373161 
TRAIN: iteration: 3500 - acc: 0.788999974728 - loss: 1.0598064661 
TRAIN: iteration: 4000 - acc: 0.794562518597 - loss: 1.02640867233 
TRAIN: iteration: 4500 - acc: 0.793500006199 - loss: 1.02963793278 
TRAIN: iteration: 5000 - acc: 0.793250024319 - loss: 1.03640413284 
TRAIN: iteration: 5500 - acc: 0.797999978065 - loss: 1.02532625198 
TRAIN: iteration: 6000 - acc: 0.797500014305 - loss: 1.00249016285 
TRAIN: iteration: 6500 - acc: 0.798937499523 - loss: 1.01074433327 
TRAIN: iteration: 7000 - acc: 0.797812521458 - loss: 1.00973689556 
TRAIN: iteration: 7500 - acc: 0.796249985695 - loss: 1.01562595367 
TRAIN: iteration: 8000 - acc: 0.801999986172 - loss: 0.981782078743 
TRAIN: iteration: 8500 - acc: 0.804187476635 - loss: 0.976303100586 
TRAIN: iteration: 9000 - acc: 0.797874987125 - loss: 1.00486862659 
TRAIN: iteration: 9500 - acc: 0.803562521935 - loss: 0.986913084984 
TRAIN: iteration: 10000 - acc: 0.79650002718 - loss: 1.01863443851 
TRAIN: iteration: 10500 - acc: 0.807250022888 - loss: 0.974831938744 
TRAIN: iteration: 11000 - acc: 0.805625021458 - loss: 0.975773096085 
TRAIN: iteration: 11500 - acc: 0.805374979973 - loss: 0.978033840656 
TRAIN: iteration: 12000 - acc: 0.80493748188 - loss: 0.973683774471 
TRAIN: iteration: 12500 - acc: 0.81387501955 - loss: 0.950825870037 
TRAIN: iteration: 13000 - acc: 0.810625016689 - loss: 0.952063202858 
TRAIN: iteration: 13500 - acc: 0.808875024319 - loss: 0.955849349499 
TRAIN: iteration: 14000 - acc: 0.810249984264 - loss: 0.942060947418 
TRAIN: iteration: 14500 - acc: 0.814999997616 - loss: 0.916026115417 
TRAIN: iteration: 15000 - acc: 0.819500029087 - loss: 0.930781543255 
TRAIN: iteration: 15500 - acc: 0.818125009537 - loss: 0.93621134758 
TRAIN: iteration: 16000 - acc: 0.820749998093 - loss: 0.92420399189 
TRAIN: iteration: 16500 - acc: 0.819812476635 - loss: 0.912742674351 
TRAIN: iteration: 17000 - acc: 0.82043749094 - loss: 0.901004970074 
TRAIN: iteration: 17500 - acc: 0.814562499523 - loss: 0.923067867756 
TRAIN: iteration: 18000 - acc: 0.822437524796 - loss: 0.886337637901 
TRAIN: iteration: 18500 - acc: 0.818562507629 - loss: 0.910832762718 
TRAIN: iteration: 19000 - acc: 0.82056248188 - loss: 0.895743668079 
TRAIN: iteration: 19500 - acc: 0.825062513351 - loss: 0.902018964291 
TRAIN: iteration: 20000 - acc: 0.818125009537 - loss: 0.914550125599 
TRAIN: iteration: 20500 - acc: 0.816562473774 - loss: 0.917225599289 
TRAIN: iteration: 21000 - acc: 0.818875014782 - loss: 0.909948706627 
TRAIN: iteration: 21500 - acc: 0.823062479496 - loss: 0.901898264885 
TRAIN: iteration: 22000 - acc: 0.820124983788 - loss: 0.89869093895 
TRAIN: iteration: 22500 - acc: 0.819374978542 - loss: 0.91407763958 
TRAIN: iteration: 23000 - acc: 0.825812518597 - loss: 0.881320655346 
TRAIN: iteration: 23500 - acc: 0.818499982357 - loss: 0.905509352684 
TRAIN: iteration: 24000 - acc: 0.824124991894 - loss: 0.889068722725 
TRAIN: iteration: 24500 - acc: 0.824000000954 - loss: 0.867092788219 
TRAIN: iteration: 25000 - acc: 0.823312520981 - loss: 0.881143331528 
TRAIN: iteration: 25500 - acc: 0.826187491417 - loss: 0.860552489758 
TRAIN: iteration: 26000 - acc: 0.827187478542 - loss: 0.869491040707 
TRAIN: iteration: 26500 - acc: 0.830624997616 - loss: 0.845009386539 
TRAIN: iteration: 27000 - acc: 0.829937517643 - loss: 0.866153240204 
TRAIN: iteration: 27500 - acc: 0.828625023365 - loss: 0.86710524559 
TRAIN: iteration: 28000 - acc: 0.825437486172 - loss: 0.893574714661 
TRAIN: iteration: 28500 - acc: 0.831749975681 - loss: 0.8528393507 
TRAIN: iteration: 29000 - acc: 0.832125008106 - loss: 0.850957155228 
TRAIN: iteration: 29500 - acc: 0.830124974251 - loss: 0.850524663925 
TRAIN: iteration: 30000 - acc: 0.830624997616 - loss: 0.8360491395 
TRAIN: iteration: 30500 - acc: 0.832187473774 - loss: 0.841859102249 
TRAIN: iteration: 31000 - acc: 0.824562489986 - loss: 0.88128888607 
TRAIN: iteration: 31500 - acc: 0.835437476635 - loss: 0.852603673935 
TRAIN: iteration: 32000 - acc: 0.837000012398 - loss: 0.826929926872 
TRAIN: iteration: 32500 - acc: 0.832687497139 - loss: 0.844348430634 
TRAIN: iteration: 33000 - acc: 0.826562523842 - loss: 0.864904046059 
TRAIN: iteration: 33500 - acc: 0.829874992371 - loss: 0.864955961704 
TRAIN: iteration: 34000 - acc: 0.82837498188 - loss: 0.876878321171 
TRAIN: iteration: 34500 - acc: 0.828750014305 - loss: 0.868550419807 
TRAIN: iteration: 35000 - acc: 0.827937483788 - loss: 0.875779211521 
TRAIN: iteration: 35500 - acc: 0.830874979496 - loss: 0.84529620409 
TRAIN: iteration: 36000 - acc: 0.82800000906 - loss: 0.849335372448 
TRAIN: iteration: 36500 - acc: 0.83581250906 - loss: 0.84064000845 
TRAIN: iteration: 37000 - acc: 0.830812513828 - loss: 0.850774824619 
TRAIN: iteration: 0 - acc: 0.875 - loss: 0.621744275093 
TRAIN: iteration: 500 - acc: 0.831875026226 - loss: 0.840247452259 
TRAIN: iteration: 1000 - acc: 0.838625013828 - loss: 0.81394970417 
TRAIN: iteration: 1500 - acc: 0.833374977112 - loss: 0.848757386208 
TRAIN: iteration: 2000 - acc: 0.833937525749 - loss: 0.839447557926 
TRAIN: iteration: 2500 - acc: 0.838874995708 - loss: 0.83003538847 
TRAIN: iteration: 3000 - acc: 0.841125011444 - loss: 0.81164842844 
TRAIN: iteration: 3500 - acc: 0.836562514305 - loss: 0.827466547489 
TRAIN: iteration: 4000 - acc: 0.838312506676 - loss: 0.791083395481 
TRAIN: iteration: 4500 - acc: 0.843812525272 - loss: 0.798357009888 
TRAIN: iteration: 5000 - acc: 0.838437497616 - loss: 0.81510078907 
TRAIN: iteration: 5500 - acc: 0.843187510967 - loss: 0.803795516491 
TRAIN: iteration: 6000 - acc: 0.845437526703 - loss: 0.78460586071 
TRAIN: iteration: 6500 - acc: 0.848187506199 - loss: 0.787241458893 
TRAIN: iteration: 7000 - acc: 0.849749982357 - loss: 0.779086947441 
TRAIN: iteration: 7500 - acc: 0.84399998188 - loss: 0.786898314953 
TRAIN: iteration: 8000 - acc: 0.847937524319 - loss: 0.773908853531 
TRAIN: iteration: 8500 - acc: 0.85118752718 - loss: 0.759711921215 
TRAIN: iteration: 9000 - acc: 0.843249976635 - loss: 0.794927120209 
TRAIN: iteration: 9500 - acc: 0.848874986172 - loss: 0.77384608984 
TRAIN: iteration: 10000 - acc: 0.843249976635 - loss: 0.784246563911 
TRAIN: iteration: 10500 - acc: 0.848375022411 - loss: 0.76851183176 
TRAIN: iteration: 11000 - acc: 0.847437500954 - loss: 0.780670166016 
TRAIN: iteration: 11500 - acc: 0.848312497139 - loss: 0.776580572128 
TRAIN: iteration: 12000 - acc: 0.849062502384 - loss: 0.77523034811 
TRAIN: iteration: 12500 - acc: 0.854562520981 - loss: 0.760008215904 
TRAIN: iteration: 13000 - acc: 0.854937493801 - loss: 0.749005615711 
TRAIN: iteration: 13500 - acc: 0.849500000477 - loss: 0.754673540592 
TRAIN: iteration: 14000 - acc: 0.852687478065 - loss: 0.748541355133 
TRAIN: iteration: 14500 - acc: 0.856062471867 - loss: 0.739295423031 
TRAIN: iteration: 15000 - acc: 0.857937514782 - loss: 0.737678945065 
TRAIN: iteration: 15500 - acc: 0.853874981403 - loss: 0.744289457798 
TRAIN: iteration: 16000 - acc: 0.863499999046 - loss: 0.720895946026 
TRAIN: iteration: 16500 - acc: 0.861625015736 - loss: 0.70686852932 
TRAIN: iteration: 17000 - acc: 0.855250000954 - loss: 0.728354513645 
TRAIN: iteration: 17500 - acc: 0.860249996185 - loss: 0.728596925735 
TRAIN: iteration: 18000 - acc: 0.860812485218 - loss: 0.7093308568 
TRAIN: iteration: 18500 - acc: 0.85962498188 - loss: 0.721703350544 
TRAIN: iteration: 19000 - acc: 0.859062492847 - loss: 0.708472251892 
TRAIN: iteration: 19500 - acc: 0.856750011444 - loss: 0.727044165134 
TRAIN: iteration: 20000 - acc: 0.858250021935 - loss: 0.733286142349 
TRAIN: iteration: 20500 - acc: 0.856062471867 - loss: 0.737380027771 
TRAIN: iteration: 21000 - acc: 0.857312500477 - loss: 0.732873558998 
TRAIN: iteration: 21500 - acc: 0.855312526226 - loss: 0.729168951511 
TRAIN: iteration: 22000 - acc: 0.857562482357 - loss: 0.72050100565 
TRAIN: iteration: 22500 - acc: 0.856999993324 - loss: 0.712022781372 
TRAIN: iteration: 23000 - acc: 0.860687494278 - loss: 0.711051404476 
TRAIN: iteration: 23500 - acc: 0.863312482834 - loss: 0.695914566517 
TRAIN: iteration: 24000 - acc: 0.859062492847 - loss: 0.71676170826 
TRAIN: iteration: 24500 - acc: 0.860000014305 - loss: 0.69733774662 
TRAIN: iteration: 25000 - acc: 0.862062513828 - loss: 0.721017956734 
TRAIN: iteration: 25500 - acc: 0.863687515259 - loss: 0.685042202473 
TRAIN: iteration: 26000 - acc: 0.862874984741 - loss: 0.691434800625 
TRAIN: iteration: 26500 - acc: 0.864875018597 - loss: 0.671208143234 
TRAIN: iteration: 27000 - acc: 0.867812514305 - loss: 0.691750705242 
TRAIN: iteration: 27500 - acc: 0.867874979973 - loss: 0.68837672472 
TRAIN: iteration: 28000 - acc: 0.864187479019 - loss: 0.713673710823 
TRAIN: iteration: 28500 - acc: 0.865625023842 - loss: 0.675118148327 
TRAIN: iteration: 29000 - acc: 0.864000022411 - loss: 0.681987166405 
TRAIN: iteration: 29500 - acc: 0.865000009537 - loss: 0.681019365788 
TRAIN: iteration: 30000 - acc: 0.870312511921 - loss: 0.674212634563 
TRAIN: iteration: 30500 - acc: 0.868624985218 - loss: 0.679089486599 
TRAIN: iteration: 31000 - acc: 0.861999988556 - loss: 0.719391167164 
TRAIN: iteration: 31500 - acc: 0.868499994278 - loss: 0.670933425426 
TRAIN: iteration: 32000 - acc: 0.872437477112 - loss: 0.665456831455 
TRAIN: iteration: 32500 - acc: 0.867937505245 - loss: 0.681746661663 
TRAIN: iteration: 33000 - acc: 0.864437520504 - loss: 0.691227972507 
TRAIN: iteration: 33500 - acc: 0.865312516689 - loss: 0.692031621933 
TRAIN: iteration: 34000 - acc: 0.863749980927 - loss: 0.702617287636 
TRAIN: iteration: 34500 - acc: 0.864499986172 - loss: 0.705823004246 
TRAIN: iteration: 35000 - acc: 0.862062513828 - loss: 0.709254682064 
TRAIN: iteration: 35500 - acc: 0.868812501431 - loss: 0.670779645443 
TRAIN: iteration: 36000 - acc: 0.866625010967 - loss: 0.688871026039 
TRAIN: iteration: 36500 - acc: 0.865499973297 - loss: 0.69660192728 
TRAIN: iteration: 37000 - acc: 0.864125013351 - loss: 0.685472786427 
TRAIN: iteration: 0 - acc: 0.875 - loss: 0.462583422661 
TRAIN: iteration: 500 - acc: 0.86706250906 - loss: 0.68967795372 
TRAIN: iteration: 1000 - acc: 0.86868751049 - loss: 0.663501322269 
TRAIN: iteration: 1500 - acc: 0.869249999523 - loss: 0.691867947578 
TRAIN: iteration: 2000 - acc: 0.866500020027 - loss: 0.67175501585 
TRAIN: iteration: 2500 - acc: 0.86756247282 - loss: 0.6776034832 
TRAIN: iteration: 3000 - acc: 0.874562501907 - loss: 0.664432823658 
TRAIN: iteration: 3500 - acc: 0.869374990463 - loss: 0.666321754456 
TRAIN: iteration: 4000 - acc: 0.87637501955 - loss: 0.643349468708 
TRAIN: iteration: 4500 - acc: 0.873374998569 - loss: 0.656932115555 
TRAIN: iteration: 5000 - acc: 0.87362498045 - loss: 0.657311856747 
TRAIN: iteration: 5500 - acc: 0.874687492847 - loss: 0.653331935406 
TRAIN: iteration: 6000 - acc: 0.875687479973 - loss: 0.638966917992 
TRAIN: iteration: 6500 - acc: 0.878062486649 - loss: 0.642227530479 
TRAIN: iteration: 7000 - acc: 0.874937474728 - loss: 0.649485349655 
TRAIN: iteration: 7500 - acc: 0.874499976635 - loss: 0.658103823662 
TRAIN: iteration: 8000 - acc: 0.879312515259 - loss: 0.635295629501 
TRAIN: iteration: 8500 - acc: 0.882624983788 - loss: 0.614451527596 
TRAIN: iteration: 9000 - acc: 0.873812496662 - loss: 0.656626701355 
TRAIN: iteration: 9500 - acc: 0.879312515259 - loss: 0.63429069519 
TRAIN: iteration: 10000 - acc: 0.87362498045 - loss: 0.648823022842 
TRAIN: iteration: 10500 - acc: 0.878062486649 - loss: 0.636593639851 
TRAIN: iteration: 11000 - acc: 0.875812470913 - loss: 0.641298353672 
TRAIN: iteration: 11500 - acc: 0.878187477589 - loss: 0.632409155369 
TRAIN: iteration: 12000 - acc: 0.878125011921 - loss: 0.632775366306 
TRAIN: iteration: 12500 - acc: 0.880437493324 - loss: 0.62685084343 
TRAIN: iteration: 13000 - acc: 0.883499979973 - loss: 0.617295622826 
TRAIN: iteration: 13500 - acc: 0.878750026226 - loss: 0.611473202705 
TRAIN: iteration: 14000 - acc: 0.882499992847 - loss: 0.620621681213 
TRAIN: iteration: 14500 - acc: 0.88131248951 - loss: 0.60656028986 
TRAIN: iteration: 15000 - acc: 0.881749987602 - loss: 0.622961163521 
TRAIN: iteration: 15500 - acc: 0.880500018597 - loss: 0.61414784193 
TRAIN: iteration: 16000 - acc: 0.884562492371 - loss: 0.601565957069 
TRAIN: iteration: 16500 - acc: 0.887312471867 - loss: 0.590665280819 
TRAIN: iteration: 17000 - acc: 0.888187527657 - loss: 0.588073194027 
TRAIN: iteration: 17500 - acc: 0.88143748045 - loss: 0.620298087597 
TRAIN: iteration: 18000 - acc: 0.885249972343 - loss: 0.589770913124 
TRAIN: iteration: 18500 - acc: 0.885500013828 - loss: 0.588838636875 
TRAIN: iteration: 19000 - acc: 0.887812495232 - loss: 0.592071771622 
TRAIN: iteration: 19500 - acc: 0.8828125 - loss: 0.602358996868 
TRAIN: iteration: 20000 - acc: 0.883875012398 - loss: 0.60497957468 
TRAIN: iteration: 20500 - acc: 0.8828125 - loss: 0.620980083942 
TRAIN: iteration: 21000 - acc: 0.880687475204 - loss: 0.607263088226 
TRAIN: iteration: 21500 - acc: 0.883499979973 - loss: 0.609869122505 
TRAIN: iteration: 22000 - acc: 0.88418751955 - loss: 0.605752229691 
TRAIN: iteration: 22500 - acc: 0.879999995232 - loss: 0.609786987305 
TRAIN: iteration: 23000 - acc: 0.8828125 - loss: 0.599583029747 
TRAIN: iteration: 23500 - acc: 0.886250019073 - loss: 0.589237272739 
TRAIN: iteration: 24000 - acc: 0.887125015259 - loss: 0.593810915947 
TRAIN: iteration: 24500 - acc: 0.886437475681 - loss: 0.587396979332 
TRAIN: iteration: 25000 - acc: 0.887312471867 - loss: 0.598614156246 
TRAIN: iteration: 25500 - acc: 0.888750016689 - loss: 0.57072520256 
TRAIN: iteration: 26000 - acc: 0.89037501812 - loss: 0.599808335304 
TRAIN: iteration: 26500 - acc: 0.888062477112 - loss: 0.554515719414 
TRAIN: iteration: 27000 - acc: 0.888437509537 - loss: 0.593120038509 
TRAIN: iteration: 27500 - acc: 0.891062498093 - loss: 0.582045435905 
TRAIN: iteration: 28000 - acc: 0.88431251049 - loss: 0.613863170147 
TRAIN: iteration: 28500 - acc: 0.888000011444 - loss: 0.575883686543 
TRAIN: iteration: 29000 - acc: 0.888812482357 - loss: 0.589133441448 
TRAIN: iteration: 29500 - acc: 0.893187522888 - loss: 0.573770701885 
TRAIN: iteration: 30000 - acc: 0.88924998045 - loss: 0.572857916355 
TRAIN: iteration: 30500 - acc: 0.884249985218 - loss: 0.595424175262 
TRAIN: iteration: 31000 - acc: 0.884937524796 - loss: 0.603139936924 
TRAIN: iteration: 31500 - acc: 0.89200001955 - loss: 0.571958065033 
TRAIN: iteration: 32000 - acc: 0.893312513828 - loss: 0.569442212582 
TRAIN: iteration: 32500 - acc: 0.888125002384 - loss: 0.583429992199 
TRAIN: iteration: 33000 - acc: 0.893812477589 - loss: 0.580403447151 
TRAIN: iteration: 33500 - acc: 0.891437470913 - loss: 0.581275939941 
TRAIN: iteration: 34000 - acc: 0.89037501812 - loss: 0.587733268738 
TRAIN: iteration: 34500 - acc: 0.89187502861 - loss: 0.596642315388 
TRAIN: iteration: 35000 - acc: 0.888499975204 - loss: 0.594966292381 
TRAIN: iteration: 35500 - acc: 0.891749978065 - loss: 0.568165361881 
TRAIN: iteration: 36000 - acc: 0.891312479973 - loss: 0.58435344696 
TRAIN: iteration: 36500 - acc: 0.888312518597 - loss: 0.591590166092 
TRAIN: iteration: 37000 - acc: 0.890562474728 - loss: 0.577033340931 
TRAIN: iteration: 0 - acc: 0.9375 - loss: 0.372843265533 
TRAIN: iteration: 500 - acc: 0.887062489986 - loss: 0.577858626842 
TRAIN: iteration: 1000 - acc: 0.893187522888 - loss: 0.555677473545 
TRAIN: iteration: 1500 - acc: 0.888437509537 - loss: 0.591990709305 
TRAIN: iteration: 2000 - acc: 0.889625012875 - loss: 0.588912308216 
TRAIN: iteration: 2500 - acc: 0.887624979019 - loss: 0.584356188774 
TRAIN: iteration: 3000 - acc: 0.892624974251 - loss: 0.582242131233 
TRAIN: iteration: 3500 - acc: 0.888062477112 - loss: 0.576491951942 
TRAIN: iteration: 4000 - acc: 0.892374992371 - loss: 0.546120405197 
TRAIN: iteration: 4500 - acc: 0.897562503815 - loss: 0.552081525326 
TRAIN: iteration: 5000 - acc: 0.896187484264 - loss: 0.561539888382 
TRAIN: iteration: 5500 - acc: 0.892374992371 - loss: 0.564218044281 
TRAIN: iteration: 6000 - acc: 0.896812498569 - loss: 0.557704567909 
TRAIN: iteration: 6500 - acc: 0.899062514305 - loss: 0.547539234161 
TRAIN: iteration: 7000 - acc: 0.895437479019 - loss: 0.561221599579 
TRAIN: iteration: 7500 - acc: 0.892937481403 - loss: 0.569300293922 
TRAIN: iteration: 8000 - acc: 0.895562529564 - loss: 0.549832046032 
TRAIN: iteration: 8500 - acc: 0.899437487125 - loss: 0.544473528862 
TRAIN: iteration: 9000 - acc: 0.896812498569 - loss: 0.54320204258 
TRAIN: iteration: 9500 - acc: 0.895624995232 - loss: 0.54953622818 
TRAIN: iteration: 10000 - acc: 0.896437525749 - loss: 0.540681421757 
TRAIN: iteration: 10500 - acc: 0.895500004292 - loss: 0.543885707855 
TRAIN: iteration: 11000 - acc: 0.896187484264 - loss: 0.558554708958 
TRAIN: iteration: 11500 - acc: 0.89868748188 - loss: 0.549283862114 
TRAIN: iteration: 12000 - acc: 0.896000027657 - loss: 0.53978651762 
TRAIN: iteration: 12500 - acc: 0.900624990463 - loss: 0.545771837234 
TRAIN: iteration: 13000 - acc: 0.900062501431 - loss: 0.543332874775 
TRAIN: iteration: 13500 - acc: 0.901250004768 - loss: 0.530563950539 
TRAIN: iteration: 14000 - acc: 0.902625024319 - loss: 0.522240877151 
TRAIN: iteration: 14500 - acc: 0.900437474251 - loss: 0.529631376266 
TRAIN: iteration: 15000 - acc: 0.901062488556 - loss: 0.542790770531 
TRAIN: iteration: 15500 - acc: 0.900437474251 - loss: 0.521715044975 
TRAIN: iteration: 16000 - acc: 0.908375024796 - loss: 0.519023835659 
TRAIN: iteration: 16500 - acc: 0.904937505722 - loss: 0.507753610611 
TRAIN: iteration: 17000 - acc: 0.901875019073 - loss: 0.523000001907 
TRAIN: iteration: 17500 - acc: 0.903750002384 - loss: 0.530667364597 
TRAIN: iteration: 18000 - acc: 0.904500007629 - loss: 0.505217552185 
TRAIN: iteration: 18500 - acc: 0.905312478542 - loss: 0.52463722229 
TRAIN: iteration: 19000 - acc: 0.901687502861 - loss: 0.523200690746 
TRAIN: iteration: 19500 - acc: 0.900125026703 - loss: 0.515866100788 
TRAIN: iteration: 20000 - acc: 0.893999993801 - loss: 0.542086780071 
TRAIN: iteration: 20500 - acc: 0.901312470436 - loss: 0.53190612793 
TRAIN: iteration: 21000 - acc: 0.904187500477 - loss: 0.531809329987 
TRAIN: iteration: 21500 - acc: 0.900562524796 - loss: 0.540723502636 
TRAIN: iteration: 22000 - acc: 0.901687502861 - loss: 0.527386009693 
TRAIN: iteration: 22500 - acc: 0.899062514305 - loss: 0.518030524254 
TRAIN: iteration: 23000 - acc: 0.900499999523 - loss: 0.529324114323 
TRAIN: iteration: 23500 - acc: 0.902062475681 - loss: 0.516202151775 
TRAIN: iteration: 24000 - acc: 0.903562486172 - loss: 0.519284307957 
TRAIN: iteration: 24500 - acc: 0.904687523842 - loss: 0.5174279809 
TRAIN: iteration: 25000 - acc: 0.902312517166 - loss: 0.525276362896 
TRAIN: iteration: 25500 - acc: 0.903625011444 - loss: 0.509291946888 
TRAIN: iteration: 26000 - acc: 0.902999997139 - loss: 0.528004646301 
TRAIN: iteration: 26500 - acc: 0.906437516212 - loss: 0.500012218952 
TRAIN: iteration: 27000 - acc: 0.904062509537 - loss: 0.515136659145 
TRAIN: iteration: 27500 - acc: 0.907062470913 - loss: 0.514987289906 
TRAIN: iteration: 28000 - acc: 0.903812527657 - loss: 0.52419346571 
TRAIN: iteration: 28500 - acc: 0.903500020504 - loss: 0.51415592432 
TRAIN: iteration: 29000 - acc: 0.903312504292 - loss: 0.511304676533 
TRAIN: iteration: 29500 - acc: 0.901624977589 - loss: 0.513156116009 
TRAIN: iteration: 30000 - acc: 0.90474998951 - loss: 0.504895448685 
TRAIN: iteration: 30500 - acc: 0.906062483788 - loss: 0.518948733807 
TRAIN: iteration: 31000 - acc: 0.901125013828 - loss: 0.533360421658 
TRAIN: iteration: 31500 - acc: 0.90487498045 - loss: 0.500902950764 
TRAIN: iteration: 32000 - acc: 0.909624993801 - loss: 0.488498151302 
TRAIN: iteration: 32500 - acc: 0.907687485218 - loss: 0.512467503548 
TRAIN: iteration: 33000 - acc: 0.90775001049 - loss: 0.494960993528 
TRAIN: iteration: 33500 - acc: 0.904687523842 - loss: 0.526476204395 
TRAIN: iteration: 34000 - acc: 0.904812514782 - loss: 0.517428159714 
TRAIN: iteration: 34500 - acc: 0.902812480927 - loss: 0.536713480949 
TRAIN: iteration: 35000 - acc: 0.904124975204 - loss: 0.52976590395 
TRAIN: iteration: 35500 - acc: 0.906437516212 - loss: 0.49901676178 
TRAIN: iteration: 36000 - acc: 0.905437529087 - loss: 0.516238033772 
TRAIN: iteration: 36500 - acc: 0.906937479973 - loss: 0.514319717884 
TRAIN: iteration: 37000 - acc: 0.904624998569 - loss: 0.501995503902 
TRAIN: iteration: 0 - acc: 0.96875 - loss: 0.297246754169 
TRAIN: iteration: 500 - acc: 0.905562520027 - loss: 0.513217508793 
TRAIN: iteration: 1000 - acc: 0.906687498093 - loss: 0.502820611 
TRAIN: iteration: 1500 - acc: 0.902062475681 - loss: 0.536316752434 
TRAIN: iteration: 2000 - acc: 0.904937505722 - loss: 0.517877399921 
TRAIN: iteration: 2500 - acc: 0.905250012875 - loss: 0.513904809952 
TRAIN: iteration: 3000 - acc: 0.909874975681 - loss: 0.50445908308 
TRAIN: iteration: 3500 - acc: 0.905562520027 - loss: 0.51583814621 
TRAIN: iteration: 4000 - acc: 0.908687472343 - loss: 0.487802267075 
TRAIN: iteration: 4500 - acc: 0.906937479973 - loss: 0.502854824066 
TRAIN: iteration: 5000 - acc: 0.908562481403 - loss: 0.50105702877 
TRAIN: iteration: 5500 - acc: 0.907999992371 - loss: 0.502296149731 
TRAIN: iteration: 6000 - acc: 0.910812497139 - loss: 0.484477251768 
TRAIN: iteration: 6500 - acc: 0.910749971867 - loss: 0.499197930098 
TRAIN: iteration: 7000 - acc: 0.910187482834 - loss: 0.494749158621 
TRAIN: iteration: 7500 - acc: 0.910250008106 - loss: 0.478944689035 
TRAIN: iteration: 8000 - acc: 0.908437490463 - loss: 0.494855314493 
TRAIN: iteration: 8500 - acc: 0.910875022411 - loss: 0.494464546442 
TRAIN: iteration: 9000 - acc: 0.909437477589 - loss: 0.492530584335 
TRAIN: iteration: 9500 - acc: 0.910374999046 - loss: 0.488806307316 
TRAIN: iteration: 10000 - acc: 0.909062504768 - loss: 0.502025127411 
TRAIN: iteration: 10500 - acc: 0.909500002861 - loss: 0.493806153536 
TRAIN: iteration: 11000 - acc: 0.908562481403 - loss: 0.494966626167 
TRAIN: iteration: 11500 - acc: 0.907875001431 - loss: 0.483579963446 
TRAIN: iteration: 12000 - acc: 0.914375007153 - loss: 0.4859816432 
TRAIN: iteration: 12500 - acc: 0.909874975681 - loss: 0.490721017122 
TRAIN: iteration: 13000 - acc: 0.911687493324 - loss: 0.485702008009 
TRAIN: iteration: 13500 - acc: 0.910875022411 - loss: 0.477321624756 
TRAIN: iteration: 14000 - acc: 0.911249995232 - loss: 0.484513312578 
TRAIN: iteration: 14500 - acc: 0.911312520504 - loss: 0.476671785116 
TRAIN: iteration: 15000 - acc: 0.914812505245 - loss: 0.482928395271 
TRAIN: iteration: 15500 - acc: 0.913437485695 - loss: 0.477290600538 
TRAIN: iteration: 16000 - acc: 0.919812500477 - loss: 0.465933591127 
TRAIN: iteration: 16500 - acc: 0.919749975204 - loss: 0.453909188509 
TRAIN: iteration: 17000 - acc: 0.915062487125 - loss: 0.469667315483 
TRAIN: iteration: 17500 - acc: 0.91443747282 - loss: 0.488862067461 
TRAIN: iteration: 18000 - acc: 0.918312489986 - loss: 0.447336494923 
TRAIN: iteration: 18500 - acc: 0.91418749094 - loss: 0.465419918299 
TRAIN: iteration: 19000 - acc: 0.918312489986 - loss: 0.456379026175 
TRAIN: iteration: 19500 - acc: 0.91256248951 - loss: 0.469634711742 
TRAIN: iteration: 20000 - acc: 0.912374973297 - loss: 0.481986761093 
TRAIN: iteration: 20500 - acc: 0.909687519073 - loss: 0.488195210695 
TRAIN: iteration: 21000 - acc: 0.914749979973 - loss: 0.479854553938 
TRAIN: iteration: 21500 - acc: 0.915812492371 - loss: 0.472381263971 
TRAIN: iteration: 22000 - acc: 0.914375007153 - loss: 0.466415077448 
TRAIN: iteration: 22500 - acc: 0.915750026703 - loss: 0.465375542641 
TRAIN: iteration: 23000 - acc: 0.916562497616 - loss: 0.459775686264 
TRAIN: iteration: 23500 - acc: 0.911562502384 - loss: 0.473776131868 
TRAIN: iteration: 24000 - acc: 0.913124978542 - loss: 0.47223892808 
TRAIN: iteration: 24500 - acc: 0.917249977589 - loss: 0.458451539278 
TRAIN: iteration: 25000 - acc: 0.911687493324 - loss: 0.478454589844 
TRAIN: iteration: 25500 - acc: 0.916312515736 - loss: 0.449944734573 
TRAIN: iteration: 26000 - acc: 0.913562476635 - loss: 0.473446398973 
TRAIN: iteration: 26500 - acc: 0.916124999523 - loss: 0.447476327419 
TRAIN: iteration: 27000 - acc: 0.915750026703 - loss: 0.468090057373 
TRAIN: iteration: 27500 - acc: 0.919187486172 - loss: 0.457492053509 
TRAIN: iteration: 28000 - acc: 0.915000021458 - loss: 0.489450931549 
TRAIN: iteration: 28500 - acc: 0.916312515736 - loss: 0.46392968297 
TRAIN: iteration: 29000 - acc: 0.916625022888 - loss: 0.458791971207 
TRAIN: iteration: 29500 - acc: 0.918375015259 - loss: 0.462189018726 
TRAIN: iteration: 30000 - acc: 0.920812487602 - loss: 0.445213109255 
TRAIN: iteration: 30500 - acc: 0.912187516689 - loss: 0.483656704426 
TRAIN: iteration: 31000 - acc: 0.914125025272 - loss: 0.476296752691 
TRAIN: iteration: 31500 - acc: 0.920249998569 - loss: 0.452851593494 
TRAIN: iteration: 32000 - acc: 0.92199999094 - loss: 0.444612801075 
TRAIN: iteration: 32500 - acc: 0.918187499046 - loss: 0.456992030144 
TRAIN: iteration: 33000 - acc: 0.918749988079 - loss: 0.472364455462 
TRAIN: iteration: 33500 - acc: 0.919875025749 - loss: 0.457647591829 
TRAIN: iteration: 34000 - acc: 0.914812505245 - loss: 0.480229556561 
TRAIN: iteration: 34500 - acc: 0.910437524319 - loss: 0.496123045683 
TRAIN: iteration: 35000 - acc: 0.913062512875 - loss: 0.471619993448 
TRAIN: iteration: 35500 - acc: 0.921312510967 - loss: 0.449161112309 
TRAIN: iteration: 36000 - acc: 0.913749992847 - loss: 0.472765803337 
TRAIN: iteration: 36500 - acc: 0.916249990463 - loss: 0.462328076363 
TRAIN: iteration: 37000 - acc: 0.91418749094 - loss: 0.467112720013 
TRAIN: iteration: 0 - acc: 0.875 - loss: 0.320620983839 
TRAIN: iteration: 500 - acc: 0.916437506676 - loss: 0.458595722914 
TRAIN: iteration: 1000 - acc: 0.918062508106 - loss: 0.458995729685 
TRAIN: iteration: 1500 - acc: 0.917874991894 - loss: 0.482797503471 
TRAIN: iteration: 2000 - acc: 0.918687522411 - loss: 0.447775542736 
TRAIN: iteration: 2500 - acc: 0.915812492371 - loss: 0.463910341263 
TRAIN: iteration: 3000 - acc: 0.919250011444 - loss: 0.46146813035 
TRAIN: iteration: 3500 - acc: 0.915875017643 - loss: 0.461359411478 
TRAIN: iteration: 4000 - acc: 0.923312485218 - loss: 0.429394990206 
TRAIN: iteration: 4500 - acc: 0.918375015259 - loss: 0.466956913471 
TRAIN: iteration: 5000 - acc: 0.918749988079 - loss: 0.450495839119 
TRAIN: iteration: 5500 - acc: 0.918124973774 - loss: 0.459632337093 
TRAIN: iteration: 6000 - acc: 0.920750021935 - loss: 0.447148233652 
TRAIN: iteration: 6500 - acc: 0.923687517643 - loss: 0.44282913208 
TRAIN: iteration: 7000 - acc: 0.919812500477 - loss: 0.454715877771 
TRAIN: iteration: 7500 - acc: 0.916937470436 - loss: 0.456656485796 
TRAIN: iteration: 8000 - acc: 0.922500014305 - loss: 0.43947738409 
TRAIN: iteration: 8500 - acc: 0.922500014305 - loss: 0.446858912706 
TRAIN: iteration: 9000 - acc: 0.919062495232 - loss: 0.458678692579 
TRAIN: iteration: 9500 - acc: 0.92325001955 - loss: 0.45170545578 
TRAIN: iteration: 10000 - acc: 0.920000016689 - loss: 0.441889405251 
TRAIN: iteration: 10500 - acc: 0.919749975204 - loss: 0.452879369259 
TRAIN: iteration: 11000 - acc: 0.917750000954 - loss: 0.447660773993 
TRAIN: iteration: 11500 - acc: 0.924749970436 - loss: 0.447540104389 
TRAIN: iteration: 12000 - acc: 0.92150002718 - loss: 0.443084657192 
TRAIN: iteration: 12500 - acc: 0.920687496662 - loss: 0.444672942162 
TRAIN: iteration: 13000 - acc: 0.922312498093 - loss: 0.434410691261 
TRAIN: iteration: 13500 - acc: 0.924312472343 - loss: 0.434177726507 
TRAIN: iteration: 14000 - acc: 0.922625005245 - loss: 0.433416455984 
TRAIN: iteration: 14500 - acc: 0.92224997282 - loss: 0.435389101505 
TRAIN: iteration: 15000 - acc: 0.924437522888 - loss: 0.448060333729 
TRAIN: iteration: 15500 - acc: 0.92312502861 - loss: 0.438552707434 
TRAIN: iteration: 16000 - acc: 0.925374984741 - loss: 0.435719907284 
TRAIN: iteration: 16500 - acc: 0.926999986172 - loss: 0.415268391371 
TRAIN: iteration: 17000 - acc: 0.928250014782 - loss: 0.429687678814 
TRAIN: iteration: 17500 - acc: 0.923562526703 - loss: 0.440384328365 
TRAIN: iteration: 18000 - acc: 0.927687525749 - loss: 0.419099062681 
TRAIN: iteration: 18500 - acc: 0.925875008106 - loss: 0.423881292343 
TRAIN: iteration: 19000 - acc: 0.925625026226 - loss: 0.425719410181 
TRAIN: iteration: 19500 - acc: 0.923562526703 - loss: 0.430863142014 
TRAIN: iteration: 20000 - acc: 0.924749970436 - loss: 0.437345534563 
TRAIN: iteration: 20500 - acc: 0.921374976635 - loss: 0.444357454777 
TRAIN: iteration: 21000 - acc: 0.920562505722 - loss: 0.45183929801 
TRAIN: iteration: 21500 - acc: 0.921687483788 - loss: 0.451350331306 
TRAIN: iteration: 22000 - acc: 0.924000024796 - loss: 0.436473727226 
TRAIN: iteration: 22500 - acc: 0.924000024796 - loss: 0.436392575502 
TRAIN: iteration: 23000 - acc: 0.920562505722 - loss: 0.432622134686 
TRAIN: iteration: 23500 - acc: 0.92199999094 - loss: 0.423172175884 
TRAIN: iteration: 24000 - acc: 0.92224997282 - loss: 0.436123669147 
TRAIN: iteration: 24500 - acc: 0.925687491894 - loss: 0.431197881699 
TRAIN: iteration: 25000 - acc: 0.925937473774 - loss: 0.437792479992 
TRAIN: iteration: 25500 - acc: 0.924374997616 - loss: 0.437168508768 
TRAIN: iteration: 26000 - acc: 0.922687470913 - loss: 0.438118040562 
TRAIN: iteration: 26500 - acc: 0.928062498569 - loss: 0.412230104208 
TRAIN: iteration: 27000 - acc: 0.924187481403 - loss: 0.428206443787 
TRAIN: iteration: 27500 - acc: 0.923312485218 - loss: 0.443406313658 
TRAIN: iteration: 28000 - acc: 0.924062490463 - loss: 0.442268371582 
TRAIN: iteration: 28500 - acc: 0.925312519073 - loss: 0.421166062355 
TRAIN: iteration: 29000 - acc: 0.923062503338 - loss: 0.436174094677 
TRAIN: iteration: 29500 - acc: 0.925312519073 - loss: 0.429175645113 
TRAIN: iteration: 30000 - acc: 0.923937499523 - loss: 0.423746466637 
TRAIN: iteration: 30500 - acc: 0.921562492847 - loss: 0.440230101347 
TRAIN: iteration: 31000 - acc: 0.92175000906 - loss: 0.453711122274 
TRAIN: iteration: 31500 - acc: 0.928749978542 - loss: 0.419459164143 
TRAIN: iteration: 32000 - acc: 0.926937520504 - loss: 0.417573362589 
TRAIN: iteration: 32500 - acc: 0.925499975681 - loss: 0.427187949419 
TRAIN: iteration: 33000 - acc: 0.927874982357 - loss: 0.420391857624 
TRAIN: iteration: 33500 - acc: 0.929125010967 - loss: 0.423094332218 
TRAIN: iteration: 34000 - acc: 0.927062511444 - loss: 0.430469721556 
TRAIN: iteration: 34500 - acc: 0.922687470913 - loss: 0.44747954607 
TRAIN: iteration: 35000 - acc: 0.92049998045 - loss: 0.449240773916 
TRAIN: iteration: 35500 - acc: 0.927500009537 - loss: 0.408249765635 
TRAIN: iteration: 36000 - acc: 0.926500022411 - loss: 0.422571837902 
TRAIN: iteration: 36500 - acc: 0.922062516212 - loss: 0.441544741392 
TRAIN: iteration: 37000 - acc: 0.924624979496 - loss: 0.421436756849 
TRAIN: iteration: 0 - acc: 1.0 - loss: 0.294075101614 
TRAIN: iteration: 500 - acc: 0.926249980927 - loss: 0.417238771915 
TRAIN: iteration: 1000 - acc: 0.926187515259 - loss: 0.416798472404 
TRAIN: iteration: 1500 - acc: 0.922874987125 - loss: 0.444113731384 
TRAIN: iteration: 2000 - acc: 0.921937525272 - loss: 0.438771247864 
TRAIN: iteration: 2500 - acc: 0.925875008106 - loss: 0.421584546566 
TRAIN: iteration: 3000 - acc: 0.925125002861 - loss: 0.430931329727 
TRAIN: iteration: 3500 - acc: 0.924437522888 - loss: 0.433537751436 
TRAIN: iteration: 4000 - acc: 0.926874995232 - loss: 0.405494451523 
TRAIN: iteration: 4500 - acc: 0.927437484264 - loss: 0.422136366367 
TRAIN: iteration: 5000 - acc: 0.926562488079 - loss: 0.41407135129 
TRAIN: iteration: 5500 - acc: 0.926874995232 - loss: 0.42035022378 
TRAIN: iteration: 6000 - acc: 0.930312514305 - loss: 0.421821892262 
TRAIN: iteration: 6500 - acc: 0.926874995232 - loss: 0.425171822309 
TRAIN: iteration: 7000 - acc: 0.92818748951 - loss: 0.412702471018 
TRAIN: iteration: 7500 - acc: 0.927562475204 - loss: 0.41470092535 
TRAIN: iteration: 8000 - acc: 0.929062485695 - loss: 0.416877985001 
TRAIN: iteration: 8500 - acc: 0.929875016212 - loss: 0.414024561644 
TRAIN: iteration: 9000 - acc: 0.929250001907 - loss: 0.414783805609 
TRAIN: iteration: 9500 - acc: 0.929125010967 - loss: 0.417122870684 
TRAIN: iteration: 10000 - acc: 0.926062524319 - loss: 0.419186085463 
TRAIN: iteration: 10500 - acc: 0.925562500954 - loss: 0.425737410784 
TRAIN: iteration: 11000 - acc: 0.926312506199 - loss: 0.419381439686 
TRAIN: iteration: 11500 - acc: 0.927187502384 - loss: 0.412659168243 
TRAIN: iteration: 12000 - acc: 0.930875003338 - loss: 0.39952865243 
TRAIN: iteration: 12500 - acc: 0.926874995232 - loss: 0.42287120223 
TRAIN: iteration: 13000 - acc: 0.928812503815 - loss: 0.423456460238 
TRAIN: iteration: 13500 - acc: 0.930812478065 - loss: 0.408665776253 
TRAIN: iteration: 14000 - acc: 0.92943751812 - loss: 0.392263174057 
TRAIN: iteration: 14500 - acc: 0.932437479496 - loss: 0.398247897625 
TRAIN: iteration: 15000 - acc: 0.928624987602 - loss: 0.427225649357 
TRAIN: iteration: 15500 - acc: 0.929750025272 - loss: 0.405155956745 
TRAIN: iteration: 16000 - acc: 0.934062480927 - loss: 0.395315974951 
TRAIN: iteration: 16500 - acc: 0.932312488556 - loss: 0.402614355087 
TRAIN: iteration: 17000 - acc: 0.929374992847 - loss: 0.40379986167 
TRAIN: iteration: 17500 - acc: 0.929374992847 - loss: 0.417586266994 
TRAIN: iteration: 18000 - acc: 0.930374979973 - loss: 0.397098392248 
TRAIN: iteration: 18500 - acc: 0.934187471867 - loss: 0.39806753397 
TRAIN: iteration: 19000 - acc: 0.934687495232 - loss: 0.396464169025 
TRAIN: iteration: 19500 - acc: 0.92956250906 - loss: 0.398002803326 
TRAIN: iteration: 20000 - acc: 0.930562496185 - loss: 0.412803590298 
TRAIN: iteration: 20500 - acc: 0.927437484264 - loss: 0.423316001892 
TRAIN: iteration: 21000 - acc: 0.929499983788 - loss: 0.411456018686 
TRAIN: iteration: 21500 - acc: 0.927500009537 - loss: 0.42061701417 
TRAIN: iteration: 22000 - acc: 0.928937494755 - loss: 0.409276783466 
TRAIN: iteration: 22500 - acc: 0.929187476635 - loss: 0.409766733646 
TRAIN: iteration: 23000 - acc: 0.931749999523 - loss: 0.401955336332 
TRAIN: iteration: 23500 - acc: 0.930124998093 - loss: 0.387776255608 
TRAIN: iteration: 24000 - acc: 0.927812516689 - loss: 0.415451288223 
TRAIN: iteration: 24500 - acc: 0.931812524796 - loss: 0.398287564516 
TRAIN: iteration: 25000 - acc: 0.931687474251 - loss: 0.41240260005 
TRAIN: iteration: 25500 - acc: 0.931437492371 - loss: 0.3940795362 
TRAIN: iteration: 26000 - acc: 0.933687508106 - loss: 0.398180782795 
TRAIN: iteration: 26500 - acc: 0.934812486172 - loss: 0.377414941788 
TRAIN: iteration: 27000 - acc: 0.930249989033 - loss: 0.404601126909 
TRAIN: iteration: 27500 - acc: 0.92993748188 - loss: 0.407200872898 
TRAIN: iteration: 28000 - acc: 0.925750017166 - loss: 0.418038874865 
TRAIN: iteration: 28500 - acc: 0.931625008583 - loss: 0.397600352764 
TRAIN: iteration: 29000 - acc: 0.930187523365 - loss: 0.40811419487 
TRAIN: iteration: 29500 - acc: 0.931999981403 - loss: 0.398315489292 
TRAIN: iteration: 30000 - acc: 0.931687474251 - loss: 0.390574544668 
TRAIN: iteration: 30500 - acc: 0.93118751049 - loss: 0.41653457284 
TRAIN: iteration: 31000 - acc: 0.929250001907 - loss: 0.422406852245 
TRAIN: iteration: 31500 - acc: 0.933624982834 - loss: 0.399408131838 
TRAIN: iteration: 32000 - acc: 0.935937523842 - loss: 0.384729802608 
TRAIN: iteration: 32500 - acc: 0.931124985218 - loss: 0.396448105574 
TRAIN: iteration: 33000 - acc: 0.932062506676 - loss: 0.405784547329 
TRAIN: iteration: 33500 - acc: 0.932312488556 - loss: 0.407966315746 
TRAIN: iteration: 34000 - acc: 0.927874982357 - loss: 0.418639212847 
TRAIN: iteration: 34500 - acc: 0.929624974728 - loss: 0.424155265093 
TRAIN: iteration: 35000 - acc: 0.929062485695 - loss: 0.408709108829 
TRAIN: iteration: 35500 - acc: 0.931937515736 - loss: 0.400192350149 
TRAIN: iteration: 36000 - acc: 0.93599998951 - loss: 0.401247054338 
TRAIN: iteration: 36500 - acc: 0.933000028133 - loss: 0.397388130426 
TRAIN: iteration: 37000 - acc: 0.932500004768 - loss: 0.395238399506 
TRAIN: iteration: 0 - acc: 0.96875 - loss: 0.153867349029 
TRAIN: iteration: 500 - acc: 0.932937502861 - loss: 0.397808730602 
TRAIN: iteration: 1000 - acc: 0.930625021458 - loss: 0.399242013693 
TRAIN: iteration: 1500 - acc: 0.927812516689 - loss: 0.423600226641 
TRAIN: iteration: 2000 - acc: 0.93093752861 - loss: 0.402095884085 
TRAIN: iteration: 2500 - acc: 0.931124985218 - loss: 0.399992614985 
TRAIN: iteration: 3000 - acc: 0.931562483311 - loss: 0.400079220533 
TRAIN: iteration: 3500 - acc: 0.932624995708 - loss: 0.400638729334 
TRAIN: iteration: 4000 - acc: 0.933812499046 - loss: 0.377066135406 
TRAIN: iteration: 4500 - acc: 0.932500004768 - loss: 0.408632159233 
TRAIN: iteration: 5000 - acc: 0.931124985218 - loss: 0.398764818907 
TRAIN: iteration: 5500 - acc: 0.933187484741 - loss: 0.404481440783 
TRAIN: iteration: 6000 - acc: 0.934312522411 - loss: 0.38805732131 
TRAIN: iteration: 6500 - acc: 0.935000002384 - loss: 0.385200619698 
TRAIN: iteration: 7000 - acc: 0.932874977589 - loss: 0.396905481815 
TRAIN: iteration: 7500 - acc: 0.931562483311 - loss: 0.394901275635 
TRAIN: iteration: 8000 - acc: 0.935750007629 - loss: 0.378363639116 
TRAIN: iteration: 8500 - acc: 0.935812473297 - loss: 0.39089858532 
TRAIN: iteration: 9000 - acc: 0.934125006199 - loss: 0.389561891556 
TRAIN: iteration: 9500 - acc: 0.934812486172 - loss: 0.387451708317 
TRAIN: iteration: 10000 - acc: 0.931687474251 - loss: 0.399270445108 
TRAIN: iteration: 10500 - acc: 0.933624982834 - loss: 0.397023797035 
TRAIN: iteration: 11000 - acc: 0.931874990463 - loss: 0.401034235954 
TRAIN: iteration: 11500 - acc: 0.936187505722 - loss: 0.385510981083 
TRAIN: iteration: 12000 - acc: 0.934312522411 - loss: 0.388266980648 
TRAIN: iteration: 12500 - acc: 0.935000002384 - loss: 0.390722453594 
TRAIN: iteration: 13000 - acc: 0.934625029564 - loss: 0.390909045935 
TRAIN: iteration: 13500 - acc: 0.934875011444 - loss: 0.381445109844 
TRAIN: iteration: 14000 - acc: 0.934562504292 - loss: 0.385187894106 
TRAIN: iteration: 14500 - acc: 0.934062480927 - loss: 0.385194897652 
TRAIN: iteration: 15000 - acc: 0.931937515736 - loss: 0.407298088074 
TRAIN: iteration: 15500 - acc: 0.936500012875 - loss: 0.378946900368 
TRAIN: iteration: 16000 - acc: 0.937687516212 - loss: 0.376142114401 
TRAIN: iteration: 16500 - acc: 0.93774998188 - loss: 0.373356372118 
TRAIN: iteration: 17000 - acc: 0.935687482357 - loss: 0.383145958185 
TRAIN: iteration: 17500 - acc: 0.936062514782 - loss: 0.381265372038 
TRAIN: iteration: 18000 - acc: 0.936812520027 - loss: 0.367625653744 
TRAIN: iteration: 18500 - acc: 0.938687503338 - loss: 0.373543083668 
TRAIN: iteration: 19000 - acc: 0.933062493801 - loss: 0.38317784667 
TRAIN: iteration: 19500 - acc: 0.937187492847 - loss: 0.367676347494 
TRAIN: iteration: 20000 - acc: 0.935437500477 - loss: 0.3832706213 
TRAIN: iteration: 20500 - acc: 0.935124993324 - loss: 0.39260917902 
TRAIN: iteration: 21000 - acc: 0.93737500906 - loss: 0.37944817543 
TRAIN: iteration: 21500 - acc: 0.933937489986 - loss: 0.400686234236 
TRAIN: iteration: 22000 - acc: 0.934562504292 - loss: 0.383844405413 
TRAIN: iteration: 22500 - acc: 0.933062493801 - loss: 0.376719474792 
TRAIN: iteration: 23000 - acc: 0.933187484741 - loss: 0.393422484398 
TRAIN: iteration: 23500 - acc: 0.938125014305 - loss: 0.371406257153 
TRAIN: iteration: 24000 - acc: 0.932937502861 - loss: 0.385753542185 
TRAIN: iteration: 24500 - acc: 0.935812473297 - loss: 0.37713226676 
TRAIN: iteration: 25000 - acc: 0.935750007629 - loss: 0.385047256947 
TRAIN: iteration: 25500 - acc: 0.936749994755 - loss: 0.375398427248 
TRAIN: iteration: 26000 - acc: 0.933000028133 - loss: 0.40050676465 
TRAIN: iteration: 26500 - acc: 0.937437474728 - loss: 0.365356206894 
TRAIN: iteration: 27000 - acc: 0.934750020504 - loss: 0.378829479218 
TRAIN: iteration: 27500 - acc: 0.934875011444 - loss: 0.385744571686 
TRAIN: iteration: 28000 - acc: 0.935312509537 - loss: 0.39004522562 
TRAIN: iteration: 28500 - acc: 0.936312496662 - loss: 0.384270399809 
TRAIN: iteration: 29000 - acc: 0.936062514782 - loss: 0.382662832737 
TRAIN: iteration: 29500 - acc: 0.935312509537 - loss: 0.37859582901 
TRAIN: iteration: 30000 - acc: 0.935625016689 - loss: 0.379923820496 
TRAIN: iteration: 30500 - acc: 0.936187505722 - loss: 0.397234320641 
TRAIN: iteration: 31000 - acc: 0.934187471867 - loss: 0.401474535465 
TRAIN: iteration: 31500 - acc: 0.936375021935 - loss: 0.377353519201 
TRAIN: iteration: 32000 - acc: 0.941749989986 - loss: 0.36436444521 
TRAIN: iteration: 32500 - acc: 0.93762499094 - loss: 0.377042979002 
TRAIN: iteration: 33000 - acc: 0.939125001431 - loss: 0.368613034487 
TRAIN: iteration: 33500 - acc: 0.935562491417 - loss: 0.385552436113 
TRAIN: iteration: 34000 - acc: 0.937187492847 - loss: 0.389331102371 
TRAIN: iteration: 34500 - acc: 0.936687529087 - loss: 0.392441898584 
TRAIN: iteration: 35000 - acc: 0.933562517166 - loss: 0.400170594454 
TRAIN: iteration: 35500 - acc: 0.940062522888 - loss: 0.36710318923 
TRAIN: iteration: 36000 - acc: 0.935750007629 - loss: 0.379110723734 
TRAIN: iteration: 36500 - acc: 0.935625016689 - loss: 0.390882700682 
TRAIN: iteration: 37000 - acc: 0.938374996185 - loss: 0.37464800477 
TRAIN: iteration: 0 - acc: 1.0 - loss: 0.209388583899 
TRAIN: iteration: 500 - acc: 0.93612498045 - loss: 0.369005650282 
TRAIN: iteration: 1000 - acc: 0.93875002861 - loss: 0.365801185369 
TRAIN: iteration: 1500 - acc: 0.931249976158 - loss: 0.404249757528 
TRAIN: iteration: 2000 - acc: 0.93712502718 - loss: 0.383434832096 
TRAIN: iteration: 2500 - acc: 0.937062501907 - loss: 0.377212464809 
TRAIN: iteration: 3000 - acc: 0.939499974251 - loss: 0.386630296707 
TRAIN: iteration: 3500 - acc: 0.936999976635 - loss: 0.387101173401 
TRAIN: iteration: 4000 - acc: 0.940500020981 - loss: 0.367378026247 
TRAIN: iteration: 4500 - acc: 0.93875002861 - loss: 0.382285952568 
TRAIN: iteration: 5000 - acc: 0.941937506199 - loss: 0.364238470793 
TRAIN: iteration: 5500 - acc: 0.940312504768 - loss: 0.376904428005 
TRAIN: iteration: 6000 - acc: 0.93737500906 - loss: 0.370325535536 
TRAIN: iteration: 6500 - acc: 0.93900001049 - loss: 0.385895818472 
TRAIN: iteration: 7000 - acc: 0.938937485218 - loss: 0.373822331429 
TRAIN: iteration: 7500 - acc: 0.937937498093 - loss: 0.374037563801 
TRAIN: iteration: 8000 - acc: 0.938125014305 - loss: 0.3765014112 
TRAIN: iteration: 8500 - acc: 0.942250013351 - loss: 0.360171616077 
TRAIN: iteration: 9000 - acc: 0.93762499094 - loss: 0.361365050077 
TRAIN: iteration: 9500 - acc: 0.937562525272 - loss: 0.371996074915 
TRAIN: iteration: 10000 - acc: 0.938374996185 - loss: 0.371575504541 
TRAIN: iteration: 10500 - acc: 0.93737500906 - loss: 0.374997913837 
TRAIN: iteration: 11000 - acc: 0.937687516212 - loss: 0.371940135956 
TRAIN: iteration: 11500 - acc: 0.939187526703 - loss: 0.378136396408 
TRAIN: iteration: 12000 - acc: 0.941250026226 - loss: 0.359893262386 
TRAIN: iteration: 12500 - acc: 0.938125014305 - loss: 0.376969337463 
TRAIN: iteration: 13000 - acc: 0.940062522888 - loss: 0.363174796104 
TRAIN: iteration: 13500 - acc: 0.938374996185 - loss: 0.370263874531 
TRAIN: iteration: 14000 - acc: 0.939437508583 - loss: 0.351590275764 
TRAIN: iteration: 14500 - acc: 0.941124975681 - loss: 0.353682518005 
TRAIN: iteration: 15000 - acc: 0.938624978065 - loss: 0.386181563139 
TRAIN: iteration: 15500 - acc: 0.938624978065 - loss: 0.357429236174 
TRAIN: iteration: 16000 - acc: 0.943187475204 - loss: 0.355686187744 
TRAIN: iteration: 16500 - acc: 0.942687511444 - loss: 0.357685774565 
TRAIN: iteration: 17000 - acc: 0.940249979496 - loss: 0.357270270586 
TRAIN: iteration: 17500 - acc: 0.938624978065 - loss: 0.377174496651 
TRAIN: iteration: 18000 - acc: 0.9453125 - loss: 0.342276126146 
TRAIN: iteration: 18500 - acc: 0.942687511444 - loss: 0.355355590582 
TRAIN: iteration: 19000 - acc: 0.938937485218 - loss: 0.36650544405 
TRAIN: iteration: 19500 - acc: 0.939562499523 - loss: 0.366329103708 
TRAIN: iteration: 20000 - acc: 0.938812494278 - loss: 0.366163313389 
TRAIN: iteration: 20500 - acc: 0.936625003815 - loss: 0.383925229311 
TRAIN: iteration: 21000 - acc: 0.939750015736 - loss: 0.371281921864 
TRAIN: iteration: 21500 - acc: 0.93887501955 - loss: 0.380864143372 
TRAIN: iteration: 22000 - acc: 0.941874980927 - loss: 0.3684438169 
TRAIN: iteration: 22500 - acc: 0.938624978065 - loss: 0.364831596613 
TRAIN: iteration: 23000 - acc: 0.941312491894 - loss: 0.358267784119 
TRAIN: iteration: 23500 - acc: 0.941624999046 - loss: 0.361398935318 
TRAIN: iteration: 24000 - acc: 0.938499987125 - loss: 0.36805549264 
TRAIN: iteration: 24500 - acc: 0.942437529564 - loss: 0.355086266994 
TRAIN: iteration: 25000 - acc: 0.939937472343 - loss: 0.372725754976 
TRAIN: iteration: 25500 - acc: 0.941062510014 - loss: 0.36326110363 
TRAIN: iteration: 26000 - acc: 0.939999997616 - loss: 0.369139194489 
TRAIN: iteration: 26500 - acc: 0.942437529564 - loss: 0.34278190136 
TRAIN: iteration: 27000 - acc: 0.941500008106 - loss: 0.366399794817 
TRAIN: iteration: 27500 - acc: 0.937687516212 - loss: 0.376214057207 
TRAIN: iteration: 28000 - acc: 0.940374970436 - loss: 0.375407934189 
TRAIN: iteration: 28500 - acc: 0.941437482834 - loss: 0.362191081047 
TRAIN: iteration: 29000 - acc: 0.943000018597 - loss: 0.367058813572 
TRAIN: iteration: 29500 - acc: 0.943687498569 - loss: 0.364037811756 
TRAIN: iteration: 30000 - acc: 0.942125022411 - loss: 0.349624603987 
TRAIN: iteration: 30500 - acc: 0.938374996185 - loss: 0.384519279003 
TRAIN: iteration: 31000 - acc: 0.940750002861 - loss: 0.371569216251 
TRAIN: iteration: 31500 - acc: 0.940312504768 - loss: 0.361539810896 
TRAIN: iteration: 32000 - acc: 0.944875001907 - loss: 0.352241843939 
TRAIN: iteration: 32500 - acc: 0.942375004292 - loss: 0.356269836426 
TRAIN: iteration: 33000 - acc: 0.941999971867 - loss: 0.362561017275 
TRAIN: iteration: 33500 - acc: 0.942187488079 - loss: 0.365692794323 
TRAIN: iteration: 34000 - acc: 0.9375 - loss: 0.369830071926 
TRAIN: iteration: 34500 - acc: 0.936749994755 - loss: 0.387635856867 
TRAIN: iteration: 35000 - acc: 0.940187513828 - loss: 0.364912539721 
TRAIN: iteration: 35500 - acc: 0.942624986172 - loss: 0.356870263815 
TRAIN: iteration: 36000 - acc: 0.93900001049 - loss: 0.376826345921 
TRAIN: iteration: 36500 - acc: 0.940812528133 - loss: 0.35944250226 
TRAIN: iteration: 37000 - acc: 0.942624986172 - loss: 0.355350017548 
TRAIN: iteration: 0 - acc: 0.96875 - loss: 0.105221547186 
TRAIN: iteration: 500 - acc: 0.941062510014 - loss: 0.356221854687 
TRAIN: iteration: 1000 - acc: 0.938812494278 - loss: 0.356365203857 
TRAIN: iteration: 1500 - acc: 0.937062501907 - loss: 0.378059774637 
TRAIN: iteration: 2000 - acc: 0.942125022411 - loss: 0.352612376213 
TRAIN: iteration: 2500 - acc: 0.93787497282 - loss: 0.360152333975 
TRAIN: iteration: 3000 - acc: 0.940874993801 - loss: 0.358233004808 
TRAIN: iteration: 3500 - acc: 0.940124988556 - loss: 0.358020454645 
TRAIN: iteration: 4000 - acc: 0.943437516689 - loss: 0.332233518362 
TRAIN: iteration: 4500 - acc: 0.937062501907 - loss: 0.361127376556 
TRAIN: iteration: 5000 - acc: 0.942749977112 - loss: 0.3527520895 
TRAIN: iteration: 5500 - acc: 0.941999971867 - loss: 0.363543212414 
TRAIN: iteration: 6000 - acc: 0.940500020981 - loss: 0.357775568962 
TRAIN: iteration: 6500 - acc: 0.942312479019 - loss: 0.354903280735 
TRAIN: iteration: 7000 - acc: 0.943374991417 - loss: 0.354625344276 
TRAIN: iteration: 7500 - acc: 0.942312479019 - loss: 0.348711133003 
TRAIN: iteration: 8000 - acc: 0.943624973297 - loss: 0.339765131474 
TRAIN: iteration: 8500 - acc: 0.94381248951 - loss: 0.348532170057 
TRAIN: iteration: 9000 - acc: 0.941624999046 - loss: 0.35460588336 
TRAIN: iteration: 9500 - acc: 0.942749977112 - loss: 0.357865959406 
TRAIN: iteration: 10000 - acc: 0.943624973297 - loss: 0.353717893362 
TRAIN: iteration: 10500 - acc: 0.939875006676 - loss: 0.365254253149 
TRAIN: iteration: 11000 - acc: 0.94493752718 - loss: 0.351937502623 
TRAIN: iteration: 11500 - acc: 0.943312525749 - loss: 0.353482782841 
TRAIN: iteration: 12000 - acc: 0.942749977112 - loss: 0.356379389763 
TRAIN: iteration: 12500 - acc: 0.939499974251 - loss: 0.3547077775 
TRAIN: iteration: 13000 - acc: 0.944625020027 - loss: 0.352424561977 
TRAIN: iteration: 13500 - acc: 0.944249987602 - loss: 0.343600928783 
TRAIN: iteration: 14000 - acc: 0.941687524319 - loss: 0.355642080307 
TRAIN: iteration: 14500 - acc: 0.944312512875 - loss: 0.348432391882 
TRAIN: iteration: 15000 - acc: 0.940500020981 - loss: 0.367890417576 
TRAIN: iteration: 15500 - acc: 0.940625011921 - loss: 0.351387709379 
TRAIN: iteration: 16000 - acc: 0.946124970913 - loss: 0.345681637526 
TRAIN: iteration: 16500 - acc: 0.945812523365 - loss: 0.343316495419 
TRAIN: iteration: 17000 - acc: 0.94381248951 - loss: 0.344599872828 
TRAIN: iteration: 17500 - acc: 0.942562520504 - loss: 0.3535579741 
TRAIN: iteration: 18000 - acc: 0.94656252861 - loss: 0.341769605875 
TRAIN: iteration: 18500 - acc: 0.944687485695 - loss: 0.349175781012 
TRAIN: iteration: 19000 - acc: 0.945874989033 - loss: 0.338395595551 
TRAIN: iteration: 19500 - acc: 0.943312525749 - loss: 0.339998573065 
TRAIN: iteration: 20000 - acc: 0.944999992847 - loss: 0.342766046524 
TRAIN: iteration: 20500 - acc: 0.942687511444 - loss: 0.356728464365 
TRAIN: iteration: 21000 - acc: 0.942687511444 - loss: 0.354039877653 
TRAIN: iteration: 21500 - acc: 0.944687485695 - loss: 0.357413679361 
TRAIN: iteration: 22000 - acc: 0.944187521935 - loss: 0.355158060789 
TRAIN: iteration: 22500 - acc: 0.94556248188 - loss: 0.347101807594 
TRAIN: iteration: 23000 - acc: 0.942687511444 - loss: 0.355974912643 
TRAIN: iteration: 23500 - acc: 0.942312479019 - loss: 0.344085752964 
TRAIN: iteration: 24000 - acc: 0.943437516689 - loss: 0.35277095437 
TRAIN: iteration: 24500 - acc: 0.944812476635 - loss: 0.347117245197 
TRAIN: iteration: 25000 - acc: 0.943687498569 - loss: 0.347440183163 
TRAIN: iteration: 25500 - acc: 0.94406247139 - loss: 0.351082712412 
TRAIN: iteration: 26000 - acc: 0.943562507629 - loss: 0.358324319124 
TRAIN: iteration: 26500 - acc: 0.944500029087 - loss: 0.334195524454 
TRAIN: iteration: 27000 - acc: 0.943499982357 - loss: 0.353610664606 
TRAIN: iteration: 27500 - acc: 0.946624994278 - loss: 0.346826612949 
TRAIN: iteration: 28000 - acc: 0.942250013351 - loss: 0.357834786177 
TRAIN: iteration: 28500 - acc: 0.942062497139 - loss: 0.353495061398 
TRAIN: iteration: 29000 - acc: 0.944437503815 - loss: 0.350843548775 
TRAIN: iteration: 29500 - acc: 0.94518750906 - loss: 0.338249027729 
TRAIN: iteration: 30000 - acc: 0.945999979973 - loss: 0.337282150984 
TRAIN: iteration: 30500 - acc: 0.942937493324 - loss: 0.358659535646 
TRAIN: iteration: 31000 - acc: 0.941999971867 - loss: 0.359152525663 
TRAIN: iteration: 31500 - acc: 0.946250021458 - loss: 0.345885068178 
TRAIN: iteration: 32000 - acc: 0.946312487125 - loss: 0.33768683672 
TRAIN: iteration: 32500 - acc: 0.943499982357 - loss: 0.357730478048 
TRAIN: iteration: 33000 - acc: 0.943562507629 - loss: 0.351627141237 
TRAIN: iteration: 33500 - acc: 0.946312487125 - loss: 0.351037949324 
TRAIN: iteration: 34000 - acc: 0.943624973297 - loss: 0.362517893314 
TRAIN: iteration: 34500 - acc: 0.941812515259 - loss: 0.363632172346 
TRAIN: iteration: 35000 - acc: 0.940812528133 - loss: 0.365514278412 
TRAIN: iteration: 35500 - acc: 0.947312474251 - loss: 0.322276502848 
TRAIN: iteration: 36000 - acc: 0.943875014782 - loss: 0.351112872362 
TRAIN: iteration: 36500 - acc: 0.94393748045 - loss: 0.354970991611 
TRAIN: iteration: 37000 - acc: 0.946874976158 - loss: 0.34507638216 
TRAIN: iteration: 0 - acc: 0.96875 - loss: 0.351451069117 
TRAIN: iteration: 500 - acc: 0.941500008106 - loss: 0.350614577532 
TRAIN: iteration: 1000 - acc: 0.940750002861 - loss: 0.347526550293 
TRAIN: iteration: 1500 - acc: 0.939312517643 - loss: 0.372653692961 
TRAIN: iteration: 2000 - acc: 0.944999992847 - loss: 0.347389519215 
TRAIN: iteration: 2500 - acc: 0.94493752718 - loss: 0.346906989813 
TRAIN: iteration: 3000 - acc: 0.946500003338 - loss: 0.357153266668 
TRAIN: iteration: 3500 - acc: 0.944249987602 - loss: 0.353791385889 
TRAIN: iteration: 4000 - acc: 0.944875001907 - loss: 0.33799764514 
TRAIN: iteration: 4500 - acc: 0.943687498569 - loss: 0.350534617901 
TRAIN: iteration: 5000 - acc: 0.947624981403 - loss: 0.340092092752 
TRAIN: iteration: 5500 - acc: 0.941749989986 - loss: 0.348749011755 
TRAIN: iteration: 6000 - acc: 0.946375012398 - loss: 0.33266595006 
TRAIN: iteration: 6500 - acc: 0.947374999523 - loss: 0.339514166117 
TRAIN: iteration: 7000 - acc: 0.944374978542 - loss: 0.347324758768 
TRAIN: iteration: 7500 - acc: 0.945249974728 - loss: 0.341552734375 
TRAIN: iteration: 8000 - acc: 0.944812476635 - loss: 0.349591851234 
TRAIN: iteration: 8500 - acc: 0.944875001907 - loss: 0.347449272871 
TRAIN: iteration: 9000 - acc: 0.945874989033 - loss: 0.340913951397 
TRAIN: iteration: 9500 - acc: 0.94393748045 - loss: 0.345630079508 
TRAIN: iteration: 10000 - acc: 0.943750023842 - loss: 0.345112860203 
TRAIN: iteration: 10500 - acc: 0.9453125 - loss: 0.341380923986 
TRAIN: iteration: 11000 - acc: 0.944374978542 - loss: 0.352340847254 
TRAIN: iteration: 11500 - acc: 0.947000026703 - loss: 0.339978635311 
TRAIN: iteration: 12000 - acc: 0.94656252861 - loss: 0.343834519386 
TRAIN: iteration: 12500 - acc: 0.94406247139 - loss: 0.351814717054 
TRAIN: iteration: 13000 - acc: 0.944812476635 - loss: 0.339088082314 
TRAIN: iteration: 13500 - acc: 0.94518750906 - loss: 0.337502926588 
TRAIN: iteration: 14000 - acc: 0.946124970913 - loss: 0.335150361061 
TRAIN: iteration: 14500 - acc: 0.947437524796 - loss: 0.328085124493 
TRAIN: iteration: 15000 - acc: 0.944812476635 - loss: 0.349440842867 
TRAIN: iteration: 15500 - acc: 0.94543749094 - loss: 0.33293735981 
TRAIN: iteration: 16000 - acc: 0.948374986649 - loss: 0.33293735981 
TRAIN: iteration: 16500 - acc: 0.948687493801 - loss: 0.322432070971 
TRAIN: iteration: 17000 - acc: 0.94668751955 - loss: 0.336532771587 
TRAIN: iteration: 17500 - acc: 0.947250008583 - loss: 0.344952195883 
TRAIN: iteration: 18000 - acc: 0.947437524796 - loss: 0.324752122164 
TRAIN: iteration: 18500 - acc: 0.947499990463 - loss: 0.331052333117 
TRAIN: iteration: 19000 - acc: 0.94668751955 - loss: 0.33511146903 
TRAIN: iteration: 19500 - acc: 0.946874976158 - loss: 0.331343621016 
TRAIN: iteration: 20000 - acc: 0.949374973774 - loss: 0.330468147993 
TRAIN: iteration: 20500 - acc: 0.943562507629 - loss: 0.346814930439 
TRAIN: iteration: 21000 - acc: 0.94568747282 - loss: 0.341882258654 
TRAIN: iteration: 21500 - acc: 0.94568747282 - loss: 0.347481489182 
TRAIN: iteration: 22000 - acc: 0.947062492371 - loss: 0.342127025127 
TRAIN: iteration: 22500 - acc: 0.94668751955 - loss: 0.33993512392 
TRAIN: iteration: 23000 - acc: 0.946250021458 - loss: 0.339280039072 
TRAIN: iteration: 23500 - acc: 0.948875010014 - loss: 0.326619327068 
TRAIN: iteration: 24000 - acc: 0.947499990463 - loss: 0.34206366539 
TRAIN: iteration: 24500 - acc: 0.945937514305 - loss: 0.326944410801 
TRAIN: iteration: 25000 - acc: 0.947624981403 - loss: 0.34195843339 
TRAIN: iteration: 25500 - acc: 0.947749972343 - loss: 0.338759958744 
TRAIN: iteration: 26000 - acc: 0.943624973297 - loss: 0.346080929041 
TRAIN: iteration: 26500 - acc: 0.947687506676 - loss: 0.330238759518 
TRAIN: iteration: 27000 - acc: 0.946749985218 - loss: 0.340119868517 
TRAIN: iteration: 27500 - acc: 0.945375025272 - loss: 0.344010144472 
TRAIN: iteration: 28000 - acc: 0.948499977589 - loss: 0.348377197981 
TRAIN: iteration: 28500 - acc: 0.947812497616 - loss: 0.331335157156 
TRAIN: iteration: 29000 - acc: 0.94656252861 - loss: 0.333770662546 
TRAIN: iteration: 29500 - acc: 0.947437524796 - loss: 0.341062247753 
TRAIN: iteration: 30000 - acc: 0.949374973774 - loss: 0.326803803444 
TRAIN: iteration: 30500 - acc: 0.946437478065 - loss: 0.350714176893 
TRAIN: iteration: 31000 - acc: 0.9453125 - loss: 0.350644856691 
TRAIN: iteration: 31500 - acc: 0.950625002384 - loss: 0.330636888742 
TRAIN: iteration: 32000 - acc: 0.949999988079 - loss: 0.32601416111 
TRAIN: iteration: 32500 - acc: 0.950437486172 - loss: 0.329870253801 
TRAIN: iteration: 33000 - acc: 0.946874976158 - loss: 0.334614127874 
TRAIN: iteration: 33500 - acc: 0.949249982834 - loss: 0.333585709333 
TRAIN: iteration: 34000 - acc: 0.947812497616 - loss: 0.344411432743 
TRAIN: iteration: 34500 - acc: 0.942187488079 - loss: 0.36372974515 
TRAIN: iteration: 35000 - acc: 0.94381248951 - loss: 0.352027773857 
TRAIN: iteration: 35500 - acc: 0.947687506676 - loss: 0.323637783527 
TRAIN: iteration: 36000 - acc: 0.947437524796 - loss: 0.336175471544 
TRAIN: iteration: 36500 - acc: 0.948374986649 - loss: 0.338446110487 
TRAIN: iteration: 37000 - acc: 0.947875022888 - loss: 0.329817503691 
TRAIN: iteration: 0 - acc: 0.96875 - loss: 0.126018822193 
TRAIN: iteration: 500 - acc: 0.947312474251 - loss: 0.328041434288 
TRAIN: iteration: 1000 - acc: 0.945500016212 - loss: 0.332846492529 
TRAIN: iteration: 1500 - acc: 0.943000018597 - loss: 0.35310921073 
TRAIN: iteration: 2000 - acc: 0.944000005722 - loss: 0.338956981897 
TRAIN: iteration: 2500 - acc: 0.944249987602 - loss: 0.339161664248 
TRAIN: iteration: 3000 - acc: 0.947499990463 - loss: 0.335476011038 
TRAIN: iteration: 3500 - acc: 0.946937501431 - loss: 0.340702176094 
TRAIN: iteration: 4000 - acc: 0.950062513351 - loss: 0.316862612963 
TRAIN: iteration: 4500 - acc: 0.947312474251 - loss: 0.331666499376 
TRAIN: iteration: 5000 - acc: 0.946874976158 - loss: 0.328570127487 
TRAIN: iteration: 5500 - acc: 0.949374973774 - loss: 0.331886291504 
TRAIN: iteration: 6000 - acc: 0.948312520981 - loss: 0.328650981188 
TRAIN: iteration: 6500 - acc: 0.948125004768 - loss: 0.339002847672 
TRAIN: iteration: 7000 - acc: 0.947875022888 - loss: 0.326016753912 
